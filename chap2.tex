\chapter{Architecture and Technologies}
\label{chap:architecture}

\section{Architecture and Data Flow}
\label{sec:architecture}
The following is the delivered architecture design of the system. The red numbers denote the flow of data
and are explained below:
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\node(P) {\includegraphics[width=1\linewidth]{figures/delivered_archi.png}};
		\tikzstyle{title} = [red]
		\node [title] at ($(P.north)+(0,0.2)$) {\Large{$(Semester~Two)$}};
	\end{tikzpicture}
	\caption{Semester 2 System Architecture.}
	\label{fig:delivered_archi}
\end{figure}

\begin{enumerate} \label{enumerate:architecture}
	\item The Gmail Scraper CLI application scrapes the business' Gmail inbox for invoices.
	      It is currently configured to accept a scrape start date (until present time) and an
	      integer value for the number of invoices to scrape (this is for testing / demo purposes).
	      The CLI application is written in Python and uses the \code{inbox}~\autocite{rusevImboxPythonIMAP} package to scrape the
	      Gmail inbox.
	      \bigbreak
	      In a production environment, this would be altered slightly and deployed as a Lambda function
	      to periodically scrape the business' Gmail inbox.
	\item The Gmail Scraper saves invoices, which match the input criteria, to a secure S3 Bucket.
	\item The scraper then sends the invoice file name, location and Bucket name to the Machine Learning Pipeline (ML Pipeline),
	      which is deployed behind a Flask server, from here on it will be referred to as the \emph{\textbf{Inference Server}}.
	\item The Inference Server pulls the desired invoice locally.
	\item The Inference Server then requests Optical Character Recognition (OCR) data for the desired invoice via an AWS Textract API call.
	      This call tells Textract the location of the invoice in the S3 Bucket and the desired region.
	      \begin{enumerate}
		      \item AWS Textract obtains the invoice from the S3 Bucket and performs OCR on the invoice.
	      \end{enumerate}
	      When it finishes, the OCR data is sent to the Inference Server.
	\item The Inference Server then prepares the OCR data for inference in a pre-process step, once this step is complete the model
	      performs the inference.\\ The results from the inference are returned, and the data then goes through a final post-process step.
	      Once the inference and post-processing are complete, and the data is in the required format, the Inference Server sends the data
	      to the Financial Server.
	\item The Financial Server is another Flask server written in Python. The server is a running service located in the Kubernetes cluster.
	      The Financial Server utilizes the SQLAlchemey~\autocite{SQLAlchemyDatabaseToolkit} \emph{Object Relational Mapper (ORM)} as a
	      \emph{translational} layer to communicate with the PostgreSQL database, also deployed in the Kubernetes cluster.
	      The Financial Server saves the data to the Financials DB.
	\item The dotted line depicts the interaction between the Kafka consumer, obtaining and saving transactional data (not operational) to the
	      Financials database.
\end{enumerate}
\subsection{The Pivot, Explained}
\label{sec:pivot}
As can be seen by comparing the proposed architecture, \Cref{fig:sys_archi}, and the delivered architecture, \Cref{fig:delivered_archi},
the system architecture has been altered. The shift may look significant, but the components are fundamentally the same. As the deployment
of a full Kubernetes environment was prohibitively expensive, the system was deployed in a Minikube cluster. This actually
increased the complexity as components to link services running locally to services running in the Minikube cluster needed to be created. \\
The change in architecture is due to the following reasons:
\begin{itemize}
	\item As mentioned, Minikube is used as the development version of Kubernetes. In essence, it is a single node Kubernetes
	      cluster\footnote{For more information see section 2.2 \emph{Technologies Used} of the semester one report}.
	      The initial architecture, as per \Cref{fig:sys_archi}, is designed to incorporate the Inference Server into the
	      Kubernetes cluster. Whilst this is still possible, as the Inference Server is containerised and \emph{Kubernetes-ready},
	      Minikube does not allow external calls from inside the Kubernetes environment. This seems like a drastic limitation and was
	      not known before the choice of Minikube as the development Kubernetes tool. Minikube will allow endpoints exposed in the
	      cluster to be accessed from outside the cluster but only from the localhost system upon which Minikube is installed.
	\item Numerous, unsuccessful attempts were made to try and circumvent this limitation of Minikube including:
	      \begin{itemize}
		      \item Configuration of a Kubernetes Ingress resource in the cluster.
		      \item The use of Ngrok on the local machine to expose the Inference Server's endpoint to the internet.
		      \item The deployment of a Ngrok pod in the cluster to expose the Inference Server's endpoint to the internet.
	      \end{itemize}
	\item The deployment of the Gmail Scraper application locally was primarily done to facilitate the demo and to aid in development. The
	      deployment of the Gmail Scraper to AWS Lambda can be achieved with a minor refactor.
\end{itemize}
As one can now visualise the data flow throughout the system components, the next step is a deeper dive into the technologies considered
for use in the system along with explanation of the chosen technologies and their implementation.
\newpage
\section{Technologies Considered for the Inference Server - AI / ML Pipeline}
\label{sec:technologies}
The Inference Server consists of the Artificial Intelligence  (AI) / Machine Learning (ML) pipeline, which is deployed behind a Flask server.
Whilst the implementation of a Flask server is trivial, the AI / ML pipeline was the most challenging component of the entire system. But also
the most interesting.\\
Other sections of this project had a large quantity of `known unknowns', this section has had a huge amount of `unknown unknowns'.
To extract desired key information from an invoice, the document must first go
through a series of steps where each step's input is dependent on the previous step's output. This is why the term used is in industry is
`pipeline'. The approach to tackling this problem must first be outlined:

\subsection{Three-Step Process}
To solve the KIE from an invoice problem, this is the three-stage process that will be used:
\begin{enumerate}
	\item \textbf{Text Localisation}: For this step a model is used to identify the location of text in the invoice. The text is
	      wrapped in bounding boxes. As per \Cref{fig:bounding_box_partial}:
	      \begin{figure}[H]
		      \centering
		      \begin{tikzpicture}[spy using outlines={circle,blue,magnification=4,size=4.2cm,line width=4pt, connect spies}]
			      \node   {\includegraphics[width=0.8\linewidth]{figures/bounding_box_partial.png}};
			      \spy[very thick] on (-1.4,-3) in node [left] at (0.8,1.25);
		      \end{tikzpicture}
		      \caption[Bounding Box Example]{An example of the bounding boxes. The locations of each word / text are detected, and a bounding box is created
			      around each piece of text. For clarity, this example has the bounding boxes drawn on. The start of each word starts with a green line and finishes with red.\\
			      \textbf{Note:} Some of the text has been removed as these are real documents which contain sensitive data.}
		      \label{fig:bounding_box_partial}
	      \end{figure}
	      This step is not the most difficult and there exist many open-source models that can achieve this with relatively good performance metrics.
	\item \textbf{Optical Character Recognition (OCR)}: For this step the bounding boxes obtained from the initial step are used by a model to extract
	      the text from the image. The text is returned in the form of a key value pair, where the key is the text and the value is the bounding box or vice versa.\\
	      This step is also not the most difficult and models exist such as Tesseract and OpenCV that can achieve this, also with relatively decent performance
	      metrics. As previously alluded to, the problem lies with the pipeline effect.
	      \bigbreak
	      If the Text Localisation stage is not successful or optimal then there is no way any subsequent step can
	      return the desired information. For example, if the Text Localisation step is 90\% accurate, The best result that can be returned from the OCR step and
	      subsequent steps is, theoretically, 90\%.\\
	      Although just `theoretically' as in practice no ML step is ever 100\% accurate, therefore, each subsequent step will bring with
	      them their \emph{`price'}, a reduction in performance.\\
	      This is why it is crucial that all steps are as accurate as possible as the third and final step is, by an order of magnitude, more difficult
	      than the previous two.
	\item \textbf{Key Information Extraction (KIE)}: This is the fascinating step. There are no real open source models, like Tesseract for OCR, of any
	      real merit for KIE. This may be because of a lack of research in general along with the variance in source data.
	      The lack of any kind of standard or structure for receipts, but in particular for invoices makes this task all the more difficult.
	      The variance in data makes it very difficult to obtain a model that is generalized (can work on all / different forms of data).\\
	      A number of different approaches / model architectures can be used to try and accomplish this step.\\
\end{enumerate}
\subsection{Visually-rich Document Understanding Competition - SROIE}
From the three stage process as outlined above, the Text Localisation and the OCR steps have both open-source and very good proprietary models.
Not to say that they are trivial, as they most certainly are not, but the main area of interest is the KIE step.\\
In general, the area of visually rich document / semi-structured document understanding is not considered a solved problem in the discipline of
computer science. To the extent that organizations exist which run competitions to try and further this field. The largest of which
is a competition that was started in 2019 by a collaboration of universities from across the globe known as the \emph{Scanned Receipts OCR
and Information Extraction (SROIE)} as part of the larger set of challenges in the area of computer vision, the 
\emph{Robust Reading Competition}~\autocite{OverviewICDAR2019}. 
This is driven by the Computer Vision Center~\autocite{ComputerVisionCenter}, a specialised research campus
in the Universitat Autonoma de Barcelona (The Autonomous University of Barcelona). Along with a host of other universities from Shanghai to
Aston to Nanyang, amongst others.
\bigbreak
The organisers for this competition created one of the first publically available and largest datasets
(of receipts) for use in this competition, known as the SROIE dataset. The competition is still ongoing, there is a leader board and there are still entries being added periodically.
The following is an example of the SROIE dataset:
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figures/sroie_example.png}
	\caption{SROIE Dataset Example (source~\autocite{ResultsICDAR2019}).}
	\label{fig:sroie_dataset_example}
\end{figure}
The SROIE competition was, initially, the main focus of research for this project and was an invaluable source for gaining a look into the cutting
edge research carried out on visually rich document understanding~\autocite{MethodStrucTexTTask}. The papers also reveal the different approaches taken by
the participating teams.
\bigbreak
The SROIE website contains links to some open-source code repos for the entries. It was the perfect place to start research and to get a better
understanding of the problem space.
\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{figures/SROIE_results.png}
	\caption[SROIE Results (KIE)]{The current results of the SROIE competition in the KIE task.\\
		\textbf{Note:} An interesting observation is that the overwhelming majority of the top end of the leaderboard are all using some variation of a model
		based on the \emph{transformer} architecture.}
	\label{fig:SROIE_results}
\end{figure}
The methods used by different teams vary greatly as can be seen in the ranking graph~\autocite{ResultsICDAR2019} by the large variation in both models
used and scores achieved.\\
Some very large and innovative tech companies have entries in the competition including Baidu, Microsoft, Tencent and Samsung to name but a few.
The dominance of the transformers based architecture in the leaderboard, suggests that these models are among the most accurate.
\bigbreak
It must be noted that the dataset differs substantially from the use case for this project. No publically available dataset (of invoices) was available for
this project, so one was created from the authors personal business.
\bigbreak
A further point of interest is that the SROIE competition requires only four fields, to be extracted. As such most projects limited their
tags (the tagged field, i.e., \code{total\_amount} for receipt total) to four fields --- company, date, address, and total. For comparison, this project
ended up with over 20 fields in order to extract the desired information.
\bigbreak
Whilst format of receipts differs, the variance is not that great. Most receipts have a similar structure. The same can not be said about
invoices. For invoices, the structure is much more varied as to are the borders / boxes / white space which separate the values.
\subsection{SROIE Models}
Whilst these differences posed challenges to completing this project, it was none-the-less decided to start trying to implement some of the open-source models
from the competition. The initial attempts proved to be extremely time-consuming and joyless. The text localisation models were attempted first. From
5 models attempted, only one was successful in deployment.
\bigbreak
The attempts at running the OCR models proved a little more successful with two of four being successfully deployed. No KIE models could be successfully
deployed from the competition.\\
There were many factors which added to the many unsuccessful attempts:
\begin{itemize}
	\item The models used varied greatly in the dependencies needed to run and the versions of the different packages used. There is a considerable
	      difference in running a model on PyTorch and TensorFlow / Keras.
	\item An initial lack of implementation / deployment experience or initial working knowledge of Python and its dependencies structure
	      increased the difficulty level.
	\item Another obstacle was that most of the repos contain comments and explanations of the code in Mandarin. This was an interesting observation.
	      The vast majority of entries were from China.\footnote{Considering the driving force is a European University and part of the funding for the competition came
		      from the EU, the overwhelming majority of the entries being from China was a surprise. That said, most of the entries in the top 10s in all three tasks
		      were Chinese. It is clear the country focuses its universities in this area.}
	\item Once the initial obstacles and challenges were cleared. The biggest limiting factor in the reproduction of the model deployment became apparent.
	      The models used by teams were trained with machines with more than the GPU memory on the development machine for this report. At 4GB of GPU memory, the
	      hardware limitations were proving to be a problem. Even with pretrained models and weights available from one or two of the repos.
\end{itemize}
Only a single Text Localisation model could be successfully run on the development machine and the other successful attempts came from running models
on AWS ec2 instances optimised for GPU memory. Although this too came with limitations as the instances with GPU access are expensive and there are no
free tier options for the hardware needed. At this point a different approach was needed.
\bigbreak
Instead of merely trying to implement the open source models as per the repo, it was decided to look at some of the top performing models and try to implement
a solution from scratch. It was during this research that the \textbf{LayoutLMv2}~\autocite{xuLayoutLMv2MultimodalPretraining2022} model was discovered. This is a newly open
sourced model, released toward the tail end of 2021 by Microsoft Azure AI~\autocite{ArtificialIntelligenceResearch}, and it showed some great promise both in
terms of performance and in terms of model size, due to the model utilizing \emph{transfer learning} (\Cref{sec:transfer_learning}). This model designed especially
for visually-rich documents.
Implementations of the original LayoutLM model were consistently near the top of the leaderboard for the KIE in the SROIE competition. As to were
other models like BERT~\autocite{BERT} and other variations of BERT like LamBERT~\autocite{LAMBERT2022} and RoBERTa~\autocite{RoBERTa}.
These models all share something in common, they are all built on the same \emph{Transformers} architecture.
% so here talk about transformers architecture and all the shite i outlined to kieran
\bigbreak
\section{Transformers and the path to LayoutLMv2}
% But first, a little bit of background as to the developments which have led to t
\bigbreak
There are not very many resources available describing how LayoutLMv2 works.\footnote{This section is by necessity quite technical, but there are a number of great
	resources to introduce this topic in more detail than what is summarized here and can be found in this excellent
	series of articles~\autocite{doshiTransformersExplainedVisually2021}.
	Some other great articles on the topic, here~\autocite{munozAttentionAllYou2021}, here~\autocite{cristinaTransformerModel2021}
	and here~\autocite{alammarIllustratedTransformer}.} Being less than a year old and dwelling in a somewhat niche
space may be some of the reasons for this.
To understand the components which constitute the LayoutLMv2 model, research of other models (LayoutLMv2 ancestors) is carried out
with information weaved together to ascertain a knowledge of the architecture and components.
\bigbreak
LayoutLMv2 is a new model which has its ancestry in the original transformers model, but
it is actually quite a hybrid of different models that have come along since the original transformers model was released~\autocite{vaswaniAttentionAllYou}.
It is not possible for me to document the entire history of the model and its ancestry in great detail, as there is simply too much information to
aggregate. Therefore, I will mention the lineage here, \Cref{fig:layoutlmv2_ancestry}, and pick out the important features which give LayoutLMv2 its extraordinary
power.
\bigbreak
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figures/layoutlmv2_ancestry.png}
	\caption{The LayoutLMv2 ancestry.}
	\label{fig:layoutlmv2_ancestry}
\end{figure}
\textbf{Note}: The models depicted in \Cref{fig:layoutlmv2_ancestry} and the order are taken from each paper which introduces
each model.\footnote{The Transformer model is introduced by this paper~\autocite{vaswaniAttentionAllYou},
	BERT by~\autocite{devlinBERTPretrainingDeep2019}, UniLM by~\autocite{dongUnifiedLanguageModel2019},
	UniLMv2 by~\autocite{baoUniLMv2PseudoMaskedLanguage2020}, LayoutLM by~\autocite{xuLayoutLMPretrainingText2020} and
	LayoutLMv2 by~\autocite{xuLayoutLMv2MultimodalPretraining2022}.}
The authors usually say that they have based the model in question on the architecture of another. Following this
line of statements, \Cref{fig:layoutlmv2_ancestry} has been produced. There are many other models which inspired components of
each model, but the main ones upon which the architecture is based are the details that are captured.
\bigbreak
The transformers architecture underpins all the models depicted in \Cref{fig:layoutlmv2_ancestry} and introduces the
important \emph{Self-Attention} mechanism which powers all of the models above. As such, this report
will outline the main concepts underpinning the transformers architecture with a particular focus
on the differences and additions that make up the LayoutLMv2 model.
\bigbreak
The Transformers architecture has revolutionized the area of Natural Language Processing (NLP) since its architecture was proposed in
the excellent paper \emph{Attention Is All You Need}~\autocite{vaswaniAttentionAllYou} developed by Vaswani et al. at Google in 2017.
\bigbreak
This architecture is used as the backbone and therefore has given rise to a number of very famous and powerful models such as the
aforementioned BERT~\autocite{BERT} and OpenAI's GPT series of models, the latest of which is the GPT-3 model~\autocite{GPT3PowersNext2021}.
The GPT-3 model has a massive variety of use cases such as English to other language translation (French, Spanish and Japanese are some of the
languages supported), Python code to Natural Language, as per \Cref{code:GPT3_NL_translation} and many others.
A more comprehensive list can be found here~\autocite{GPT3PowersNext2021}.
\newpage
\begin{lstlisting}[language=python, label={code:GPT3_NL_translation}, caption={GPT-3 Python code for human language translation as per~\autocite{OpenAIAPI}.}]
def remove_common_prefix(x, prefix, ws_prefix): 
    x["completion"] = x["completion"].str[len(prefix) :] 
    if ws_prefix: 
        # keep the single whitespace as prefix 
        x["completion"] = " " + x["completion"] 
return x 

# Output generated by GPT-3:
# The code above is a function that takes a dataframe and a prefix as input and returns a dataframe with the prefix removed from the completion column.
\end{lstlisting}
\bigbreak
\textbf{Note}: Not all the output is as coherent and accurate as the chosen example, although a sub-product of GPT-3 can be found powering tools
such as GitHub Co-Pilot which is a tool to aid developers by making suggestions for code completion based on the context of the code already written or
by the comments written in the code which Co-Pilot can then use to suggest code completion and / or code generation with very decent results in real time.\\
This is just a single example of the type of applications for these type of models.
\bigbreak
Before Vaswani's paper, the state-of-the-art models for NLP were Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) which implemented an
\emph{encoder} and a \emph{decoder}. 
\bigbreak
These types are known as sequence to sequence models (seq2seq) as a sequence is used as the input and output.
For example, Machine Translation (MT) is a seq2seq model where the input is a string of words and the output is a translation in another language for the
input string. This was the initial use case for the Transformers architecture, although English to French and French to German were the chosen
languages\footnote{The Greek translation is pronounced `Yasu Cosme'}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/basic_transformer.png}
	\label{fig:basic_transformer}
	\caption{Basic Transformer Sequence to Sequence example.}
\end{figure}
\newpage
\subsection{Encoder and Decoder High Level Overview}
Peeking under the hood, we can see that the encoder and decoder are responsible for the translation. The original paper proposed a stack of
six encoders and similarly a matching set stack of six decoders, although these numbers can be altered.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/transformer_stacked_encoder_decoder.png}
	\label{fig:transformer_stacked_encoder_decoder}
	\caption{Transformer Stacked Encoder and Decoder example (source~\autocite{alammarIllustratedTransformer}).}
\end{figure}
% fix this.
All the encoders are identical in structure, but they have their own weights associated to them. At first these weights are set at random, but they are
altered through the back-propagation of the training phase.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/encoder_architecture.png}
	\caption{Transformer Encoder architecture example (source~\autocite{alammarIllustratedTransformer}).}
	\label{fig:transformer_encoder}
\end{figure}
The Self-Attention layer will be described in more detail shortly (\Cref{sec:attention}) but for now lets look at the decoder architecture:
\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{figures/decoder_archi_1.png}
	\caption{Transformer Decoder architecture example (source~\autocite{alammarIllustratedTransformer}).}
	\label{fig:transformer_decoder}
\end{figure}
As is observed the decoder is almost identical to the encoder save for an added \code{Encoder-Decoder} layer. This layer
\emph{helps} the decoder to `focus' on the relevant part of the input sequence. It does this by masking the positions in the sequence
that are not in current focus.
\subsection{Transformers Architecture}
% fix this
The level of detail is about to increase, as to is the complexity, but I've tried to keep the detail to
a minimum whilst covering the concepts needed to understand how transformers work.\\
Now that we have a very high level idea of the main components of the Transformers architecture, we can look at the actual architecture of
the Transformers model. To start \Cref{fig:transformer_architecture} shows the inner workings of the encoder on the left, and then the decoder on the right.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.55\linewidth]{figures/original_transformer_archi.png}
	\caption[Original Transformers Architecture~\autocite{vaswaniAttentionAllYou}]{Original Transformers architecture~\autocite{vaswaniAttentionAllYou}, akin to the previous example the encoder stack
		is situated on the left whilst the decoder is situated on the right.}
	\label{fig:transformer_architecture}
\end{figure}
\begin{itemize}
	\item \textbf{Black Arrows}: In~\Cref{fig:transformer_architecture}, the black arrows depict the dataflow.
	\item \textbf{Input Embeddings}: As can be seen at the bottom of the~\Cref{fig:transformer_architecture}, the \code{Inputs} are fed in to the encoder side and creates the
	      \code{Input Embedding}. \code{Outputs} flow into the decoder side and create the \code{Output Embedding}. This is only during inference. In training
	      the model works differently as the output is known and as such that known sequence is fed in to the decoder side. Part of the output embedding
	      is masked during training so the model doesn't `peek ahead'.
	      \bigbreak
	      In the original transformer architecture the input embeddings are a vector of a fixed size (this usually varies from model to model).
	      The input vectors combine the input sequence, actually a `tokenized' sequence (see \Cref{sec:tokenisation}) combined with the positional data
	      (1-D) of that particular token in the input sequence.
	\item \textbf{Add \& Norm}: The \code{Add \& Norm} refer to the addition of weights and a normalisation function, which uses `layer normalization'~\autocite{vijayraniaDifferentNormalizationLayers2021} to normalise.
	\item \textbf{Multi-Head Attention}: The \code{Multi-Head Attention} is the heart of the transformer. It is essentially numerous self attention layers stacked together
	(\Cref{sec:attention}).
	\item\textbf{Linear}: There are two linear translations in the \code{Linear} component which directly proceed a softmax function.
	\item \textbf{Feed Forward}: The feed forward neural network is a stack of layers. An input layer, some hidden layers and an output layer.
	      The data never flows backwards only forwards. The goal of the feed forward network is to approximate some function of the input.
	      \begin{figure}[H]
		      \centering
		      \includegraphics[width=1\linewidth]{figures/feed_forward.png}
		      \label{fig:feed_forward}
		      \caption{Classic Feed Forward Neural network layer architecture~\autocite{guptaDeepLearningFeedforward2018}, The left side depicts
			      a single perceptron whilst the right depicts a Multi-Layer Perceptron network.}
	      \end{figure}
	      They are also known as a \emph{Multi-Layer Perceptron (MLP)}. One of the first and most popular deep learning models~\autocite{FeedforwardNeuralNetwork2022}.
	\item \textbf{Softmax}: The Softmax function is used to compress the outputs to form a number in the range 0 - 1,
	      whose values all add up to 1. These values can now be thought of as probabilities, these are the \code{Output Probabilities}.
	\item Output Probabilities determine the token for that position. The token sequence is then sent back around to the start of the decoder stack.
	\item They are shifted right by one position as a special kind of token to indicate the start of a sequence is prepended to the sequence.
	\item The process is repeated until the end of the sequence is reached - for inference, or until the epochs are completed - for training.
\end{itemize}

\subsection{Context}
\label{sec:context}
Human languages are a beautiful construct. The ability to express complex ideas and meanings to each other is fundamental
to our species evolution, both technical and cultural. But they are also incredibly complex to learn. There are many different syntaxes, rules and
of course, rule breakers. A word can take on a range of different meanings depending on the context (a homonym). \\
For example:
\begin{enumerate}
	\item \emph{The sound of a dog bark startled the cat.}
	\item \emph{The cat scampered up the tree bark.}
\end{enumerate}
The word \emph{bark} has two different meanings depending on the context. Humans are quite good at being able to tell which meaning
should be derived from the context, but trying to teach this to a machine is a much more complex task.
\bigbreak
This idea of context in a sentence as above can be thought of as the different geographical location of text blocks on a document,
in particular an invoice. To be able to extract the relevant meaning, the context is vital. As aforementioned, invoices
have no set standard and can vary significantly. During the creation of the dataset for this project an invoice was discovered
with two varying styles of expression for two vary similar products. It is one of three to four test invoices used during system
development. A brief look through other documents corroborated that this type of finding is quite common across a multitude of
different suppliers invoices. This is an unfortunate trend.
\bigbreak
Both highlighted rows are identical in every field, except for the description.\\
The beers are even made by the same company, yet the description of the quantity was, presumably, left to be filled in by two different humans with
no structure or naming standards. With such inconsistencies in the data, the task of learning becomes more difficult and the concept of context
becomes extremely important.
\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{figures/invo_complexity.png}
	\caption[Invoice depicting inconsistencies]{This is a real invoice depicting inconsistencies in the description, highlighted in green.}
	\label{fig:context}
\end{figure}
For optimal performance, a model would need to have a way of associating different parts of an invoice with similar parts of other invoices,
throughout different layouts to establish a pattern. The model would need to pay particular attention to the context of the data.
\bigbreak
To overcome the complexities of the human language, some pre-processing must first be performed on the text data to convert it to
numbers which the model can use to manipulate and ultimately learn from. The way that transformers based models remember the
distances between words, in sentences or what their \emph{closeness} / association is to other words by using the attention mechanism.
\bigbreak
To understand what that is, it is helpful to understand the data which flows into the model as the input. We have already
briefly touched on the \code{embedding} procedure for the original transformer model, but now we will look at it in more detail
as we compare it to LayoutLMv2. Possessing the knowledge of what data is in the input embeddings (and output embeddings) will allow us to
understand what the attention mechanism is doing.
\bigbreak
The main text model input string is not a sequence of words, but a sequence of \emph{tokens}.
\subsection{Tokenisation}
\label{sec:tokenisation}
To improve performance a body of text is first tokenized, or split into smaller chunks.\\
Tokenization usually comes in three different forms; there are word, subword and character-based tokenization methods~\autocite{WhatTokenizationTokenization2020}.
\begin{itemize}
	\item \textbf{Word Tokenization}: A word is defined as a sequence of characters which are separated by a delimiter, usually, separated by a space.
	      This method has some drawbacks, like when the model encounters \emph{Out of Vocabulary words (OOV)}, these are words that the model has not encountered
	      in training and as such do not appear in the vocabulary.\\ There are some ways to deal with OOV words, but they are not very performative.
	      \bigbreak
	      A further issue is the size of the vocabulary. Pre-trained models, such as the transformers based models, are usually trained on a massive corpus of
	      data. As each unique encountered word is stored, the model size quickly explodes.
	\item \textbf{Character-Based Tokenisation}: A character-based tokenization is a method of tokenizing a text document by splitting the text into
	      individual characters. \\
	      The downside of character-based tokenization is that the model is not able to learn the meaning of the words as there are far more combinations of
	      individual characters than that of words. So detecting a pattern between these characters is extremely difficult. The vocabulary size is only
	      26.
	\item \textbf{Subword Tokenization}: A subword tokenization is a method of tokenizing a text document by splitting the text into subwords.
	      For example, the word \emph{smartest} is split into \emph{smart} and \emph{est}, whilst the word \emph{largest} is split into \emph{large} and \emph{st}.
	      The subword tokenization method is very performative, as it is able to learn the meaning of the words. If the model encounters an OOV word, it
	      will break it down and may learn meaning from the subwords and the subwords proximity to and from other words. \\
	      It is important to note that
	      this method will not split every word into subwords. Frequently occurring words are kept as is, whilst words that occurr less frequently
	      are split. This is highly dependent on the corpus, but as an example, the word \emph{annoy} will not be split, if the word \emph{annoying}
	      occurs frequently in the corpus then \emph{annoying} will also be kept. If however, it is rare in the corpus then it will be split
	      into \emph{annoy} and \emph{ing}. If they model encounters an OOV word such as \emph{annoyingly}. It can make sense of the word by splitting it
	      into \emph{annoy}, \emph{ing} and \emph{ly} then it will find subwords which match.
	      \bigbreak
	      There are many subword tokenization methods, but two shall be explained for relevance.
	      \begin{itemize}
		      \item \textbf{Byte-Pair Encoding (BPE)}: is a popular and performative method that initializes the vocabulary to include
		            every character present in the corpus, and each set of characters' (words) frequency is determined.
		            BPE then counts the frequency of each possible symbol pair that occurs most frequently and merges
		            them together. This merge strategy is repeated until stopped by the user. It is a tunable hyperparameter.\\
		            This is easier explained by looking at an example, as per the Hugging Face documentation~\autocite{SummaryTokenizers}:
		            \bigbreak
		            This is the corpus and frequency:\\
		            \code{("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)}\\
		            Which gives us this base vocabulary:\\
		            \code{["b", "g", "h", "n", "p", "s", "u"]}\\
		            The first symbol pair chosen is: \\
		            \code{("u" + "g")}\\
		            As this combination appears most 10 + 5 + 5 = 20.\\
		            The symbol pair is added to the vocabulary: \\
		            \code{["b", "g", "h", "n", "p", "s", "u", "ug"]}
		            \bigbreak
		            The merging process is repeated until the user defined limit of the vocabulary size or number of merges is reached.
		      \item \textbf{WordPiece Tokenization}: is a subword tokenization method which is used in the LayoutLMv2 model. It is based on and is very similar to
		            the BPE method. The difference is that the BPE method chooses the most frequent symbol pair, whilst the WordPiece method uses probabilities.\\
		            As per our example:\\
		            \code{"u"} followed by \code{"g"} would only have merged if the probability of \code{"ug"} divided by \code{"u"}, \code{"g"} was greater
		            than any other pair.
	      \end{itemize}
\end{itemize}
Most of the Transformers architecture based models use BPE or some variation of it.
\subsection{Multi Modal Token Embeddings}
These tokens are given numerical IDs which are kept in a look-up table.\\
The tokens are not the only data fed into the model. The \emph{multi-modality} refers to a combination of other data types. With the original
transformers architecture the 1-D position or position in a sequence is combined with the original input token.
\bigbreak
In the case of LayoutLMv2, the multi-modality is the combination of the token and the position the initial training phase.
This is known as the \emph{Layout Embedding} and refers to the geometrical position of the token in the document.\footnote{The
	positional encoding is done in a two step process using a $sin$ for the first step and a $cos$ for the second stop. More details
	on this can be found here~\autocite{doshiTransformersExplainedVisually2021c}.}
As this is a requirement and a large part of the reason that this model is so performative, a list of bounding boxes must be provided to the model
upon calling it for training or inference. \\
The tokenizer process determines the bounding box (from the initial OCR'd input bounding boxes for words) per token in the tokenization stage.
\bigbreak
Furthermore, in LayoutLMv2 segment embeddings are present. These have been picked up from BERT and are not present in the
original Transformers model. They are used to distinguish different text segments. These segments can be thought of as a block of text
\emph{t} long. Each token that is tokenized from the OCR input is assigned to a segment.
\bigbreak
After this, some special tokens are inserted, along with the text tokens. Each sequence is prepended with a \code{[CLS]}, token
whilst the end of each segment is denoted with a \code{[SEP]}. If the sequence is shorter than the max sequence length, then
a number of \code{[PAD]} tokens are inserted to bring the minimum sequence length. We will see the use cases for these tokens
later.
\bigbreak
A further addition to the LayoutLMv2 model which again differs from the original transformers model is further input into the final embedding which
will traverse the system. This input is a visual representation AKA an \emph{image embedding} of the token~\autocite{LayoutLMExplained2022}.
As the document in JPG or PNG format is also passed in as a parameter when calling the model, the model is able to slice out the token locations
from the document. This is achieved in parallel to the token and position encoding via a \emph{Faster R-CNN}~\autocite{FasterRCNNExplained2020}.
LayoutMLv2 (the Hugging Face version) uses Detectron2~\autocite{FacebookresearchDetectron22022} as the Faster R-CNN which essentially acts as the model's visual backbone and
object detection algorithm.
\bigbreak
The combination of tokens, positional data in both the 1-D sequence and 2-D the entire document, along with the visual aspect provided by the
Detectron2 and the segment embeddings are the final embedded input matrices. As detailed, these models are known as multi-modal as there
are a variety of input mediums that constitute the embeddings.\\
It is important to note that embedding only occurs once, in the initial input stage of the model.

\subsection{LayoutLMv2 Architecture}
These state-of-the-art models must not be treated as back boxes. We have already learned that our own biases get baked into models, when these models
are deployed in situations where their consequences has proven to be huge\footnote{As is the case with the \emph{Correctional Offender Management
		Profiling for Alternative Sanctions (COMPAS)} model~\autocite{rahmanCOMPASCaseStudy2020}, which was / is used in multiple states in the USA.
	This model has been shown to have an inherit racial bias and recommend African Americans have a higher chance of re-offending than whites and
	therefore should get stronger sentences.
	This model does not take into account generational racism along with the difficulty an African American from a disadvantaged area has in life.}, every care
should be taken to ensure that the models output can be questioned and the factors driving the decision should be available. The models and the
people who use them should be accountable for their actions.\\
It is for this reason that the architecture will be explained in as fine a detail as can be.
\bigbreak
% talk about attention here, from a human context
Having gained a greater understanding of the input for the LayoutLMv2 model we observe the architecture\footnote{The difference between the original LayoutLM and LayoutLMv2
	is that the visual and positional embedding occurs only during the fine-tuning phase for the original LayoutLM, whereas, LayoutLMv2 includes this
	data during the initial training phase too.}.
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figures/layoutlmv2_archi.png}
	\caption{LayoutLMv2 Architecture (source~\autocite{xuLayoutLMv2MultimodalPretraining2022}).}
	\label{fig:layoutlmv2_archi}
\end{figure}
Observing \Cref{fig:layoutlmv2_archi}, there are still many components which we have not dived into. We have defined the
layers which make up the embedding input and now, having covered the basic transformer layers architecture,
our attention can turn to a deep dive into the \emph{Transformers Layers}, from \Cref{fig:layoutlmv2_archi}, and in
particular the self attention mechanism.

% \subsection{Multi-head Self Attention, Is All You Need}
\subsection{Attention, Is All You Need}
\label{sec:attention}
As humans, we are subjected to an enormous amount of information every second we are conscious. Smells, sounds,
visuals, interactions with objects, environmental factors like heat or rain. We obviously have a limited capacity to store
this information, so how does the brain decide which bits of information should be stored as a memory and which
bits can be discarded?
\bigbreak
Attention, appears to be a key factor of what constitutes criteria for memory creation. We can all remember important life events,
celebrations and sadness. As we engage in the moment and the moment draws our full attention, we have a higher chance of remembering
the moment. Most people can remember details from the far past of events like weddings, births, etc., yet we collectively struggle to find our sets of keys or phones.
\bigbreak
Previous state-of-the-art models have had limited \emph{memory}, how far back in time (or how far back a sequence of tokens).
RNNs and CNNs have a limited capacity to remember relationships between tokens further back in a sequence. Long-Short-Term-Memory models
improve upon RNNs and CNNs, but they too suffer from the same problem.\\
\bigbreak
Just as attention is a key factor of memory for humans, can the same be said for ML models?
\bigbreak
Attention is the key component of the transformer architecture\footnote{Attention is also one of the more difficult concepts to grasp. These resources
	played a pivotal role in acquiring an understanding~\autocite{doshiTransformersExplainedVisually2021b}, here~\autocite{doshiTransformersExplainedVisually2021a},
	here~\autocite{cristinaTransformerAttentionMechanism2021}.
	and here~\autocite{alammarIllustratedTransformer}.}.
Remember the difficulties surrounding human language context?~(\Cref{sec:context}) This is the layer that attempts to capture and learn
the complexities of that context.
\bigbreak
We will try to get an intuition for attention via the lens of our use case, trying to predict the labels for text in an invoice,
we'll focus on what happens per individual item of text and try to develop an intuition as to how attention is used in
transformers based models.\\
As an example we will use the \emph{total figure} value, which is always on an invoice. When the model tries to classify the total figure,
let's call this the Query, with the correct label, the model looks at every other text
block in the invoice and tries to determine if there is any similarity or closeness between every \emph{i-th} word in the invoice and the Query. This
method of paying attention to every word, \emph{i}, with respect to itself (the Query) is why it is called \emph{\textbf{self}-attention}.\\
The way that the similarity between words is determined, is by way of a score that every \emph{i'th} word has in relation to the Query.\\
It tries to use the context of the surrounding text to determine which label should be predicted.
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figures/attention_total_invoice.png}
	\caption{Attention example for the total figure in an invoice.}
	\label{fig:attention_total_invoice}
\end{figure}
From \Cref{fig:attention_total_invoice}, the Query or the desired \emph{total figure} is circled in red.
\begin{itemize}
	\item The words `INVOICE TOTAL' are circled in green, these words would be given a very high score as this combination of words appears
	      quite frequently and in a similar geometric position relational to the Query. If one was looking for the `total value' then looking for
	      `INVOICE TOTAL' is a good place to start.
	\item The words circled in blue, would be given high scores too, but not as high as the words in green. That makes sense, as words like these,
	      `total', `VAT' and `Nett', can often be found in close to proximity to the Query.
	\item The purple words are examples of words that will have a low score towards the Query. They are further away geometrically and the
	      words are not likely to be repeated in a similar position in other invoices (from a different supplier) that the model has seen for training.
\end{itemize}
In training the model has access to the entire annotated invoice.\\
At the start the training, model will make a pretty random prediction as it hasn't learned anything yet. When the time comes to evaluate its prediction,
if the prediction is wrong the model may determine that it has paid too much attention to the purple encircled words, when those words brought little value to making
an accurate prediction, the scores for these words, in relation to the Query, will be reduced. The text enclosed in the blue and green circles will
see their scores increase. \\
Therefore, in the next training round or for inference the model will have a better understanding, by virtue of these scores, of the text to pay attention
to when predicting the label / classifying the token. The multiple epochs of training allows these scores to be compounded and the model (should) get a little
better each time.
\bigbreak
The concept of \emph{Attention} is used in the Transformers model in three places as per \Cref{fig:attention_usage}:
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/attention_usage.png}
	\caption[Attention Usage]{Attention is used in three places from here~\autocite{doshiTransformersExplainedVisually2021a}.}
	\label{fig:attention_usage}
\end{figure}
\begin{itemize}
	\item Self-Attention in the Encoder.
	\item Self-Attention in the Decoder.
	\item Self-Attention in the Encoder-Decoder, also in the Decoder.
\end{itemize}
But how are the scores calculated? Well these \emph{Attention Scores} are calculated in the \emph{Attention Layers} of the model.
The attention layer has three input parameters namely, the $Query (Q), Key (K), Value (V)$. All three of these parameters
are matrices of the same size. The concept of the $Q$, $K$ and $V$ can be thought of as a retrieval process, explained excellently here~\autocite{emilAnswerWhatExactly2020}.
\bigbreak
As an example, when one types a \textbf{query} to search for a recipe for cheesecake on Google search, the search engine maps the query against a set
of \textbf{keys} --- article title, description, words in articles (cheese, cake), ratings, etc., associated with candidate websites.
The search engine then returns a list of websites (\textbf{value}) which match the initial query.
% \bigbreak
% From the input sequence every single word (actually embedding with the positional encoding, but it is easier to think 
% of it as a word and will be referred to as such in this section) in the sequence is represented as a vector. 
% This vector 

% In practice, this would be just like a single self-attention layer. As a Multi-head attention layer is used in the 
% LayoutLMv2 model architecture, we first need to define the self-attention. 

% this would be analogous to concatenating other word vectors to create a matrix. This is the difference between a regular self attention layer
% and a multi-head attention layer, the increase in dimensionality. This increase in dimensionality brings with it many performance improvements, the model
% can train and infer quicker. Along with more accurate results, higher dimensionalities allow for more details and relationships to be recorded.\\
% For the sake of clarity we will initially reduce the dimensionality and describe a single sequence matrix, made up of word vectors.\\
% This gives a matrix of size (Sequence Length, Embedding Size, \sout{Sample Size}) - as we have reduced dimensionality.
\bigbreak
We will look at the self-attention layer in the encoder.
\subsubsection{Encoder}
The Encoder is key to LayoutLMv2. Another feature inherited from BERT, is that there are no Decoder layers present in the architecture.
There are only stacked encoders. We will see shortly how this operates.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{figures/encoder_close_up.png}
	\caption[Encoder Self-Attention]{Encoder Self-Attention (source~\autocite{vaswaniAttentionAllYou}).}
	\label{fig:encoder_close_up}
\end{figure}
What does the Encoder do? What is its role? At a high level the Encoder takes in an input sequence and maps it to an abstract
continuous representation that contains all the learned information for that sequence.
It does this in a series of steps.
\subsubsection{Query, Key \& Value}
The first step is to create a \emph{Query}, \emph{Key} and a \emph{Value} matrix for each word in the input sequence. This is done by feeding the input sequence into
three distinct fully connected linear layers. The $Q$, $K$ and $V$ matrices are multiplied by their
respective weights matrices.
\bigbreak
% The result is K, Q and V. They are updated via the three separate linear layers
% as per \Cref{fig:kvq_ll}. Each linear layer has its own weights~\autocite{alammarIllustratedTransformer}.
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figures/k_v_q_linear_layers.png}
	\caption{Key, Value and Query creation via linear layers (source~\autocite{doshiTransformersExplainedVisually2021a}).}
	\label{fig:kvq_ll}
\end{figure}
\subsubsection{Attention Score}
The next stop is to calculate the attention scores. This is the formula used:

\[
	Z =
	\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]
\bigbreak
Where:
\begin{itemize}
	\item $Q$ is the query matrix.
	\item $K$ is the key matrix.
	\item $V$ is the value matrix.
	\item $QK^{T}$ is the dot product of the query matrix and the transpose of the key matrix.
	\item $d_{k}$ is the dimensionality of the key matrix.
\end{itemize}
A nice way to visualise this can be seen in the figure~\cref{fig:attention_score_matrix_form}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/attention_formula_matrix_form.png}
	\caption{Attention Score Formula visualized in the context of matrices (source~\autocite{alammarIllustratedTransformer}).}
	\label{fig:attention_score_matrix_form}
\end{figure}
We will now walk through the formula step by step:
\begin{enumerate}
	\item The Query and the Key undergo a dot product, and the result is a score matrix. This matrix determines the amount of focus each word should put on
	      another word. The higher the score, the higher the focus.
	      \bigbreak
	      From our invoice example, how much focus should the model place on the `INVOICE TOTAL' words in relation to determining the label for
	      the total invoice figure.
	\item As these numbers may be quite high, they need to be scaled down. This is done by dividing by $QT^{T}$ by the square
	      root of the dimension of the key matrix. This step also increases efficiency.
	\item The resulting matrix goes through a softmax function. This transforms arbitrary input values
	      so that the output is in the range 0\ldots1. Effectively turning the values to probabilities.\\
	      Using a softmax function has two advantages:
	      \begin{itemize}
		      \item It is computationally safer, and it leads to more robust algorithm, to compute.
		      \item It also has a conceptual advantage since we can treat the output as probabilities.
	      \end{itemize}
	      These are known as the attention weights.
	\item The attention weights are then multiplied by the value matrix. The resulting matrix is the output. The higher
	      softmax scores from the previous step will keep the important words and the lower softmax scores will drown out the
	      less relevant words.
	\item The output layer, Z, then goes through a linear layer to process. Z is our attention score matrix.
\end{enumerate}
To recap attention, at a very high level attention acts as a windowing processes facilitating the ability of the model to focus on
a subset of the given data. The attention score is a metric of relevance / proximity between two words.
\bigbreak
As this is a multi-headed attention module the input, $Q$, $K$ and $V$ are split up before they go through the self-attention module,
and the attention weights are calculated for each head\footnote{The Transformers architecture has 8 attention heads whilst
	BERT has variations with 12 or 24 attention heads.}.
Within the attention layer multiple attention scores are calculated in parallel for performance reasons. They are logical
partitioned. i.e., they do not share data and run separately and run on separate CPU /GPU cores.\\
There is a further reason. In theory, each head should learn something different. As each set of weights starts from a different
randomized matrix, the use of the loss function\footnote{A loss function is used in the process of the model changing weights, to get
	a more accurate outcome, during training. The model makes predictions and the error for those predictions is calculated. It is this
	\emph{loss} that we are trying to minimize. It can be seen as a search / optimization problem. We need to find values for weights
	which reduce the loss, therefore, giving more accurate prediction.
	\bigbreak
	Cross-entropy loss is used in the
	transformers models. With entropy being defined from the field of information Theory~\autocite{brownleeGentleIntroductionCrossEntropy2019},
	Cross entropy loss multiplies the true value by the log of its predicted values, it tries to penalise a classifier more
	when the classifier misclassifies with a high probability. That is if one prediction is 100 and the other is 10, if
	the true answer is 2 then the classifier which predicted the 100 will be penalised more than the classifier
	which predicted 10.}
should each explore a different space. The outputs of each head are then concatenated together to form
the final output, which goes through the last linear layer. This gives the model more representation power.
\bigbreak
As we now have our output from the attention layer, the next step is a Residual Connection. This can be seen in the
\Cref{fig:encoder_close_up}, as the arrow which starts before the attention layer, skips it and goes straight to the add norm.
This process sees the original input added to the output of the attention layer.
The resulting matrix is then `layer normalized'. Layer normalization normalizes each feature to a mean of zero and unit variance,
this helps the model network produce better, smoother gradients in the next weight
update~\autocite{vijayraniaDifferentNormalizationLayers2021} which also help to reduce the training time.
This is the \textbf{Add \& Norm} component.
\bigbreak
The normalized output is then fed forward into the \textbf{Feed Forward} component, made up of two linear translation layers.
Residual connections follow again as the output from the Feed Forward is added and normalized to the normalized output of the
attention layer.\\
These residual connections help the network train by allowing gradients to flow through the network.


% In training the values for the linear layers are initially set randomly and are updated during training to return the
% final weights\footnote{This depends on the training, pre-training or fine-tuning, these concepts will be covered in detail later.}. \\
% For the multi-head attention layer the matrices are \textbf{logically} split up and distributed across the heads to allow for computation in parallel.
% Choosing a \emph{query size} parameter determines the size of the logical partitioning.
% \bigbreak
% The important part to note here is that each input word has gone through a series of transformations. Position encoding, Embedding and the Linear
% Layers.\\
% Each of these translations is tunable, and it is how the model learns. When a predicted output is wrong, the weights are altered to reflect the
% wrong decision. Similarly, for correct output in the training phase, the weights are altered to reflect the correct decision.
\subsection{LayoutLMv2 Pre-Training Tasks}
\textbf{Note}: As mentioned the LayoutLMv2 architecture does not have any decoders. It uses the \emph{pre-training tasks}
to both train and predict.
\bigbreak
The pre-training tasks used in the LayoutLMv2 architecture are heavily influenced from BERT~\autocite{devlinBERTPretrainingDeep2019}.
With some additional modifications to allow for the positional encodings and visual information to be included.
There are three pre-training tasks, of which we are mostly concerned with the first two.\\
First it is useful to define some special case tokens which have not been covered
thus far:
\begin{itemize}
	\item \textbf{[BOS]}: Beginning of sentence.
	\item \textbf{[EOS]}: End of sentence.
	\item \textbf{[UNK]}: Out-of-vocabulary tokens
	\item \textbf{[PAD]}: When a sequence size is smaller than expected constant sequence size padding is added. The model
	      knows to ignore the padding during processing due to its special type.
	\item \textbf{[CLS]}: Is used to initialize the sequence and is used as the token or label to predict, it is always
	      prepended to the input sequence. It stands for `classification'.\\
	      This is the output that will be put through a softmax function, the probabilities for each word in
	      vocabulary are calculated. The highest probability becomes the prediction.
	      \bigbreak
	      In our use case for inference. The \code{[CLS]} will be the label that the model predicts. For each input sequence the model looks `around'
	      the invoice. It notices the relationship between every word in the sequence and the invoice. The model then uses the
	      highest attention score as the output.
	\item \textbf{[MASK]}: To mask tokens for pre-training and inference.
	\item \textbf{[SEP]}: Separates questions and answers~\autocite{devlinBERTPretrainingDeep2019}.
\end{itemize}
\subsubsection{Masked Visual-Language Modeling (MVLM)}
During training an input sequence is fed into the model. Some tokens in the input sequence are masked. The model tries
to predict the masked tokens and fill in the sequence using the context of every token with every other token. We
calculate the Cross-Entropy loss, adjust the weights and the cycle continues until epochs are complete.\\
If we applied no masking then the model would predict the entire sequence with 100\% accuracy as it has its entire input sequence.
\bigbreak
This is achieved by masking 15\% of the input sequence. Of this 15\%~\autocite{devlinBERTPretrainingDeep2019}:
\begin{enumerate}
	\item 80\% of the tokens are masked with the \code{[MASK]} token.
	\item A further 10\% are given a new token at random from the entire model vocabulary. This allows the model to deal
	      with words that it hasn't seen, in the input sequence. It also allows the model to learn the context of the tokens with
	      respect to all tokens.
	\item 10\% are left back as is.
\end{enumerate}
It is important to note that the layout information (positional encodings) remain unchanged. This step is crucial
as it allows the model to learn from the positional elements.\\
The image regions are masked before they are fed to the visual encoder to avoid visual leakage.
\subsubsection{Text-Image Alignment (TIA)}
This pre-training task is a novel task, that the writers proposed in the paper~\autocite{xuLayoutLMv2MultimodalPretraining2022}.
This task aids the model to learn the spatial location context between the image and the bounding boxes.\\
In this task some of the images are randomly selected, and their image regions are covered on the document image.
During pre-training, a classification layer is built above the encoder outputs. This layer predicts a label for
each text token depending on whether it is covered, i.e., [Covered] or [Not Covered]\footnote{The paper authors use
	the \code{[Covered]} or \code{[Not Covered]} to avoid confusion with the MLVM \code{[MASK]} tokens.}, and
computes the binary cross-entropy loss.
\subsubsection{Text-Image Matching (TIM)}
Text-Image Matching is applied to help the model learn relationships between the text and
the document image. The output representation from the \code{[CLS]} token is fed into a classifier. This
prediction is used to determine if the text and the image come from the same document.

\bigbreak
The authors of the LayoutLMv2 paper tested differing versions of the model using all combinations of pre-training tasks. The
combination of combining these three tasks has given the best results.
% % talk about it in the context of invoicing.
% The scorer mechanism is used in the encoder and twice in the decoder. The attention the same in
% First this report will tackle the attention in the encoder. The attention used in the decoder is very similar to the attention in the encoder. \\
%  explain this
\subsection{Fine-Tuning}
In \Cref{sec:transfer_learning}, \emph{Transfer Learning} is discussed where the time-consuming and expensive
training of large models is broken in two stages:
(expensive) pre-training on large dataset to gain generalised language capabilities and (much cheaper) fine-tuning.\\
The pre-training tasks are used to train the model to gain a generalized understanding of language and context. That is,
the model has more capacity to learn specifics needed for a task.
\bigbreak
Then for the fine-tuning tasks, a few task-specific layers are added. In our use case, this is token classification.
The authors of the LayoutLMv2 paper do not document the process for token classification, using BERT as the
example we can get a rough visualisation of the inputs and outputs during this task.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/bert_token_classification.png}
	\caption{The input and output of the BERT token classification layer (source~\autocite{devlinBERTPretrainingDeep2019}).}
	\label{fig:token_classification_bert}
\end{figure}
The \code{[CLS]} token is inserted to denote the start of the sequence, the labels and their positions are also passed into the model.
The model learns which labels should be predicted for each token, through the back propagation of the weights. The model is given
a few new layers that sit on top of the hidden layer text which specifies for which kind of fine-tune task the model is required
to learn.


\subsection{Transfer Learning}
\label{sec:transfer_learning}
% show the problem and why transfer learning is the solution
The advancement in machine learning in recent years has been nothing short of remarkable. We have some fantastic models with near
human level performance at some tasks, but these models take a huge amount of resources to train. From time training to the huge amount of
compute resources and by virtue, energy. If every one that uses a model has to start from scratch then that can only be viewed as a devastating
waste of resources.
\bigbreak
Transfer learning is a technique which allows a model to be trained on a dataset or numerous datasets to be used as its base. The initial
training of these models still requires a huge amount of data and compute power along with expertise in the domain, but once the model has been
\emph{pre-trained} it can be used as a starting point for model on a new task. The pre-training phase usually includes similar but not identical
data sets to allow for a model that is more generalized, that is more readily applicable to a variety of different tasks.
\bigbreak
In essence, transfer learning is a more human way of dealing with knowledge. It allows for knowledge that has been gained from a particular set
of tasks to be stored, distributed and applied to other tasks. When a child learns the physics of real world interactions by walking, playing,
observing how objects move through space, etc., it uses this base knowledge to shortcut the learning process
when learning new but related things.
\bigbreak
The following are some examples are intended to give the reader an appreciation of the complexities and costs
associated with training modern large models:
\begin{itemize}
	\item The \emph{BigScience Large Language Model}~\autocite{BigScienceResearchWorkshop}
	      is a model that is currently in training. The training of BigScience's main model started on March 11, 2022 11:42am PST and will continue
	      for 3-4 months on 384 A100 80GB GPUs of the \emph{Jean Zay} public supercomputer~\autocite{BigscienceTr11176BmllogsHugging}. The data set
	      contains 341.6 billion tokens. which is approximately 1.5 TB of data.\\ That is an astonishing amount of compute, data and engineering expertise.
	\item The GPT3 model costs between 10 and 20 million dollars to train~\autocite{aiGPT3NoLonger}.
	\item Google's DeepMind model that was used to learn the Chinese board game Go, is estimated to have
	      cost a staggering 35 million dollars to train~\autocite{AIWeeklyAI2021}.
	\item Microsoft and Nvidia's Megatron-Turing language model, one of the largest of its kind, is estimated to cost in the
	      millions of dollars to training~\autocite{MicrosoftNvidiaTeam2021}.
\end{itemize}
These examples are some of the more extreme, but these `more extreme' examples are becoming more and more common place. \\
These examples show the scale of what the initial base training can entail.
\bigbreak
In the case of LayoutLMv2, the base model has been trained on six different datasets, from the
visually-rich document understanding area including the aforementioned, SROIE dataset and others including the FUNSD dataset~\autocite{FUNSD}
and the CORD dataset~\autocite{CORDConsolidatedReceipt2022}.
\bigbreak
These models are then \emph{fine-tuned} on custom datasets for the specific task at hand. For example, LayoutLMv2 in this instance is
fine-tuned on the custom invoice dataset which is optimized for invoices, but the same base model would also be used, in a specialised
fine-tuned version for receipts and other documents which see a drastic shift in document layout. \\
A model optimized for invoices may still work for some receipts, but to obtain
state-of-the-art results, the model should be fine-tuned on the dataset for which it will be used to infer.\\
A downstream task\footnote{A downstream task can be thought of as a models capabilities or different implementations, for example,
	LayoutLMv2 can be used for token classification (like this project) but it can also be used for \emph{Sequence Classification}
	and \emph{Question Answering} tasks~\autocite{LayoutLMV2}.} to first classify what structure the document is and then use a trained model that is optimized for
that document structure could be implemented to further increase the accuracy of the system.
\bigbreak
The fine-tuning phase takes a lot less resources. It can be done on most mid-range machines. The addition of a GPU greatly 
increases the efficacy of the training, if utilized. 
\bigbreak
This idea of transfer learning cuts out the large training time and allows most anyone to use and fine-tune a model to their specific needs.
It allows for the implementation of models which would traditionally have been cost-prohibitive to implement. Transfer Learning 
should lead to surge in innovation as more people are exposed to more powerful models. The budget for training LayoutLMv2 has not been 
disclosed, but preliminary research indicates that the cost of training LayoutLMv2 runs into the thousands, maybe more.
\bigbreak
Some libraries now exist which contain not only the pretrained models, but datasets of varying sizes and forms, along with other components
to aid in the pre-processing of the data to be used in the fine-tune training model phase.\\
The \emph{transformers library} by Hugging Face~\autocite{HuggingFaceAI} is one such library.
\newpage
\section{Hugging Face}
% stick that in a footnote
Hugging Face~\autocite{HuggingFaceAI} is a French company which initially developed a chat app that has since pivoted into becoming one of the most
popular deep learning model platforms~\autocite{syalHuggingFaceStep2020}. Here is what they say about themselves:
\begin{quoting}
	Hugging Face Transformers provides APIs to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute
	costs, carbon footprint, and save you time from training a model from scratch. The models can be used across different modalities such as:
	text, audio, images, video, and multimodal.
	\begin{flushright}
		~---~Hugging Face Team
	\end{flushright}
\end{quoting}
They specialise in Natural Language Processing (NLP) and they have a large number of models available for public use.\\
The company provide the \code{Transformers} library which contains over thirty pre-trained base models available
in over 100 languages~\autocite{HuggingFaceTransformers}. The models are mostly of based on the transformers architecture.\\
Hugging Face also provide \emph{The Model Hub}~\autocite{ModelsHuggingFace}, this works akin to a code repository for models, once a model is uploaded to the hub, it can be
made public or private. Hugging Face also provide a large variety of open-source datasets~\autocite{HuggingFaceAIa} which can be used to train models.\\
The team have a massive amount of tutorials and guides on model architecture available along with a large number of articles, blogs and open-sourced code for
model evaluation, training and inference.
\bigbreak
They make money via their Inference API where there are a plethora of models ready behind endpoints for a variety of different use cases.
They also charge for support and for deployment and training of models on their servers.
\bigbreak
The Transformers library is a very powerful library, it looks to cut away at the \emph{boiler-plate} code required to implement a model.\\
The library supports the PyTorch, TensorFlow and JAX frameworks. It even supports cross framework deployment so a model may be trained using
PyTorch and deployed using TensorFlow or JAX for inference. All cross framework integrations appear to be supported~\autocite{Transformers}.\\
The library also contains some great functions for data pre-processing:
\begin{itemize}
	\item \textbf{Tokenizer}: The pre-processing step to convert text into a sequence of tokens.
	\item \textbf{FeatureExtractor}: This function resizes images for use in models and can even be used with the OCR engine enabled (PyTesseract).
	      Although this method is quite basic and is not suitable for more complex tasks such as the token classification task in this project.
	\item \textbf{Processor}: This is a model specific function which wraps the tokenizer and the feature extractor in to a single function for
	      ease of use.
\end{itemize}
The tokenizer is utilized in this project.
The transformers library along with the available resources on the Hugging Face website were used extensively during the research and development
phases of the project.