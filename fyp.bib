
@misc{aiGPT3NoLonger,
  title = {{{GPT-3}} Is {{No Longer}} the {{Only Game}} in {{Town}}},
  author = {in AI, Last Week},
  abstract = {GPT-3 was by far the largest AI model of its kind last year. Now? Not so much.},
  howpublished = {https://lastweekin.ai/p/gpt-3-is-no-longer-the-only-game},
  file = {/home/dimdakis/Zotero/storage/8G3P2IC3/gpt-3-is-no-longer-the-only-game.html}
}

@misc{AIWeeklyAI2021,
  title = {{{AI Weekly}}: {{AI}} Model Training Costs on the Rise, Highlighting Need for New Solutions},
  shorttitle = {{{AI Weekly}}},
  year = {2021},
  month = oct,
  journal = {VentureBeat},
  abstract = {AI models are becoming larger -- and more capable. But they're also becoming more costly to train, highlighting a problem in the field.},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/8GJ9FPG6/ai-weekly-ai-model-training-costs-on-the-rise-highlighting-need-for-new-solutions.html}
}

@misc{alammarIllustratedTransformer,
  title = {The {{Illustrated Transformer}}},
  author = {Alammar, Jay},
  abstract = {Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Russian, Spanish, Vietnamese Watch: MIT's Deep Learning State of the Art lecture referencing this post In the previous post, we looked at Attention \textendash{} a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \textendash{} a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud's recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let's try to break the model apart and look at how it functions. The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard's NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter. 2020 Update: I've created a ``Narrated Transformer'' video which is a gentler approach to the topic: A High-Level Look Let's begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
  howpublished = {https://jalammar.github.io/illustrated-transformer/},
  file = {/home/dimdakis/Zotero/storage/2VK4QG4A/illustrated-transformer.html}
}

@misc{amamouFineTuningLayoutLMV22022,
  title = {Fine-{{Tuning LayoutLM}} v2 {{For Invoice Recognition}}},
  author = {Amamou, Walid},
  year = {2022},
  month = apr,
  journal = {Medium},
  abstract = {From annotation to training and inference},
  howpublished = {https://towardsdatascience.com/fine-tuning-layoutlm-v2-for-invoice-recognition-91bf2546b19e},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/6SBXMRRI/fine-tuning-layoutlm-v2-for-invoice-recognition-91bf2546b19e.html}
}

@misc{amamouFineTuningTransformerModel2021,
  title = {Fine-{{Tuning Transformer Model}} for {{Invoice Recognition}}},
  author = {Amamou, Walid},
  year = {2021},
  month = aug,
  journal = {Medium},
  abstract = {A step-by-step guide from annotation to training},
  howpublished = {https://towardsdatascience.com/fine-tuning-transformer-model-for-invoice-recognition-1e55869336d4},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/HZJMP67M/fine-tuning-transformer-model-for-invoice-recognition-1e55869336d4.html}
}

@misc{ApacheKafka,
  title = {Apache {{Kafka}}},
  journal = {Apache Kafka},
  abstract = {Apache Kafka: A Distributed Streaming Platform.},
  howpublished = {https://kafka.apache.org/documentation/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/RJWJFNSN/documentation.html}
}

@misc{ApacheKafkaKubernetes2020,
  title = {Apache {{Kafka}} on {{Kubernetes}} with {{Strimzi}} - {{Part}} 3: {{Monitoring}} Our {{Strimzi Kafka Cluster}} with {{Prometheus}} and {{Grafana}}},
  shorttitle = {Apache {{Kafka}} on {{Kubernetes}} with {{Strimzi}} - {{Part}} 3},
  year = {2020},
  month = oct,
  journal = {Sina Nourian},
  abstract = {Deploying Apache Kafka on Kubernetes with Strimzi; Producing and Consuming messages with Go and Scala; Monitoring with Prometheus and Grafana},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/UAX2JDNZ/kafka-kubernetes-strimzi-part-3-monitoring-strimzi-kafka-with-prometheus-grafana.html}
}

@misc{APIFlaskDocumentation,
  title = {{{API}} \textemdash{} {{Flask Documentation}} (2.1.x)},
  howpublished = {https://flask.palletsprojects.com/en/2.1.x/api/\#flask.Request.json}
}

@misc{APIReferenceOverview,
  title = {{{API Reference Overview}}},
  howpublished = {https://docs.clover.com/reference/api-reference-overview\#},
  file = {/home/dimdakis/Zotero/storage/LIG9DL9I/api-reference-overview.html}
}

@misc{ArtificialIntelligenceResearch,
  title = {Artificial {{Intelligence}} Research at {{Microsoft}} Aims to Enrich Our Experiences},
  journal = {Microsoft Research},
  abstract = {Microsoft AI Research is creating artificial intelligence machines that complement human reasoning to augment and enrich our experience and competencies.},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/VSWUL5WX/artificial-intelligence.html}
}

@misc{bachinaHowUseOwn2020,
  title = {How to {{Use Own Local Docker Images With Minikube}}},
  author = {Bachina, Bhargav},
  year = {2020},
  month = jan,
  journal = {Bachina Labs},
  abstract = {A step by step guide with an example project},
  langid = {english}
}

@article{baoUniLMv2PseudoMaskedLanguage2020,
  title = {{{UniLMv2}}: {{Pseudo-Masked Language Models}} for {{Unified Language Model Pre-Training}}},
  shorttitle = {{{UniLMv2}}},
  author = {Bao, Hangbo and Dong, Li and Wei, Furu and Wang, Wenhui and Yang, Nan and Liu, Xiaodong and Wang, Yu and Piao, Songhao and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.12804 [cs]},
  eprint = {2002.12804},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM). Given an input text with masked tokens, we rely on conventional masks to learn inter-relations between corrupted tokens and context via autoencoding, and pseudo masks to learn intra-relations between masked spans via partially autoregressive modeling. With welldesigned position embeddings and self-attention masks, the context encodings are reused to avoid redundant computation. Moreover, conventional masks used for autoencoding provide global masking information, so that all the position embeddings are accessible in partially autoregressive language modeling. In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively. Our experiments show that the unified language models pre-trained using PMLM achieve new state-of-the-art results on a wide range of natural language understanding and generation tasks across several widely used benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/dimdakis/Zotero/storage/K3MBAMPS/Bao et al. - 2020 - UniLMv2 Pseudo-Masked Language Models for Unified.pdf}
}

@misc{BERT,
  title = {{{BERT}}},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/model\_doc/bert},
  file = {/home/dimdakis/Zotero/storage/5UBZCPS2/bert.html}
}

@misc{BERTNaturalLanguage,
  title = {{{BERT}} for {{Natural Language Processing}} |{{All You Need}} to Know about {{BERT}}},
  abstract = {BERT stands for Bidirectional Encoder Representation from Transformer and uses deep bidirectional layers of transformers encoders},
  howpublished = {https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-bert/}
}

@misc{BestCDCTools,
  title = {The 7 {{Best CDC Tools}} ({{Change Data Capture}}) - {{Learn}} | {{Hevo}}},
  howpublished = {https://hevodata.com/learn/7-best-cdc-tools/}
}

@misc{BigScienceResearchWorkshop,
  title = {{{BigScience Research Workshop}}},
  howpublished = {https://bigscience.huggingface.co/},
  file = {/home/dimdakis/Zotero/storage/ENHTNJZL/bigscience.huggingface.co.html}
}

@misc{BigscienceTr11176BmllogsHugging,
  title = {Bigscience/Tr11-{{176B-ml-logs}} {$\cdot$} {{Hugging Face}}},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/bigscience/tr11-176B-ml-logs},
  file = {/home/dimdakis/Zotero/storage/3EXBMSJY/tr11-176B-ml-logs.html}
}

@misc{brownleeEncoderDecoderRecurrentNeural2017,
  title = {Encoder-{{Decoder Recurrent Neural Network Models}} for {{Neural Machine Translation}}},
  author = {Brownlee, Jason},
  year = {2017},
  month = dec,
  journal = {Machine Learning Mastery},
  abstract = {The encoder-decoder architecture for recurrent neural networks is the standard neural machine translation method that rivals and in some cases [\ldots ]},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/GQ9YK3AY/encoder-decoder-recurrent-neural-network-models-neural-machine-translation.html}
}

@misc{brownleeGentleIntroductionCrossEntropy2019,
  title = {A {{Gentle Introduction}} to {{Cross-Entropy}} for {{Machine Learning}}},
  author = {Brownlee, Jason},
  year = {2019},
  month = oct,
  journal = {Machine Learning Mastery},
  abstract = {Cross-entropy is commonly used in machine learning as a loss function. Cross-entropy is a measure from the field of information [\ldots ]},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/43N5BIGB/cross-entropy-for-machine-learning.html}
}

@misc{calvoDissectingBERTPart2019,
  title = {Dissecting {{BERT Part}} 1: {{The Encoder}}},
  shorttitle = {Dissecting {{BERT Part}} 1},
  author = {Calvo, Miguel Romero},
  year = {2019},
  month = may,
  journal = {Dissecting BERT},
  abstract = {This is Part 1/2 of Understanding BERT written jointly by Miguel Romero and Francisco Ingham. If you already understand the Encoder\ldots},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/X5M86LSF/dissecting-bert-part-1-d3c3d495cdb3.html}
}

@misc{ClickUpOneApp,
  title = {{{ClickUp}}\texttrademark{} | {{One}} App to Replace Them All},
  abstract = {Our mission is to make the world more productive. To do this, we built one app to replace them all - Tasks, Docs, Goals, and Chat.},
  howpublished = {https://clickup.com},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/IX9VE5PD/clickup.com.html}
}

@misc{cloudfactoryDataAnnotationTools,
  title = {Data {{Annotation Tools}} for {{Machine Learning}}: {{An Evolving Guide}}},
  shorttitle = {Data {{Annotation Tools}} for {{Machine Learning}}},
  author = {CloudFactory},
  abstract = {Selecting the right tool to annotate your data can save you time, money, and frustration. This guide will help you understand how to choose the best data annotation tools for your machine learning project.},
  howpublished = {https://www.cloudfactory.com/data-annotation-tool-guide},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/8ZSS247W/data-annotation-tool-guide.html}
}

@misc{codeemporiumBERTNeuralNetwork2020,
  title = {{{BERT Neural Network}} - {{EXPLAINED}}!},
  author = {{CodeEmporium}},
  year = {2020},
  month = may,
  abstract = {Understand the BERT Transformer in and out. Please subscribe to keep me alive: https://www.youtube.com/c/CodeEmporiu... INVESTING [1] Webull (You can get 3 free stocks setting up a webull account today): https://a.webull.com/8XVa1znjYxio6ESdff REFERENCES [1] BERT main paper: https://arxiv.org/pdf/1810.04805.pdf [1] BERT in google search: https://blog.google/products/search/s... [2] Overview of BERT: https://arxiv.org/pdf/2002.12327v1.pdf [4] BERT word embeddings explained: https://medium.com/@\_init\_/why-bert-h... [5] More details of BERT in this amazing blog: https://towardsdatascience.com/bert-e... [6] Stanford lecture slides on BERT: https://nlp.stanford.edu/seminar/deta...}
}

@misc{ComputerVisionCenter,
  title = {Computer {{Vision Center}} | {{The Computer Vision Centre}} ({{CVC}}) Is a Not for Profit Institute, Leader in Research and Development in the Field of Computer Vision},
  abstract = {The Computer Vision Centre (CVC) is a not for profit institute, leader in research and development in the field of computer vision},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/W6A38UW9/www.cvc.uab.es.html}
}

@misc{Concepts,
  title = {Concepts},
  journal = {Kubernetes},
  abstract = {Production-Grade Container Orchestration},
  howpublished = {https://kubernetes.io/docs/concepts/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/TGL364UJ/concepts.html}
}

@misc{ConnectorsDebeziumDocumentation,
  title = {Connectors :: {{Debezium Documentation}}},
  howpublished = {https://debezium.io/documentation/reference/stable/connectors/index.html},
  file = {/home/dimdakis/Zotero/storage/8VC4E2Y4/index.html}
}

@misc{CORDConsolidatedReceipt2022,
  title = {{{CORD}}: {{A Consolidated Receipt Dataset}} for {{Post-OCR Parsing}}},
  shorttitle = {{{CORD}}},
  year = {2022},
  month = mar,
  abstract = {CORD: A Consolidated Receipt Dataset for Post-OCR Parsing},
  copyright = {CC-BY-4.0},
  howpublished = {Clova AI Research}
}

@misc{cristinaTransformerAttentionMechanism2021,
  title = {The {{Transformer Attention Mechanism}}},
  author = {Cristina, Stefania},
  year = {2021},
  month = oct,
  journal = {Machine Learning Mastery},
  abstract = {Before the introduction of the Transformer model, the use of attention for neural machine translation was being implemented by RNN-based [\ldots ]},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/KSGA89ZB/the-transformer-attention-mechanism.html}
}

@misc{cristinaTransformerModel2021,
  title = {The {{Transformer Model}}},
  author = {Cristina, Stefania},
  year = {2021},
  month = nov,
  journal = {Machine Learning Mastery},
  abstract = {We have already familiarized ourselves with the concept of self-attention as implemented by the Transformer attention mechanism for neural machine [\ldots ]},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/SUYYIEZ5/the-transformer-model.html}
}

@misc{DD5D5V08elb,
  title = {{{DD5D5V08elb}}},
  howpublished = {https://link.medium.com/DD5D5V08elb}
}

@misc{DebeziumArchitectureDebezium,
  title = {Debezium {{Architecture}} :: {{Debezium Documentation}}},
  howpublished = {https://debezium.io/documentation/reference/architecture.html},
  file = {/home/dimdakis/Zotero/storage/VE2SE9E7/architecture.html}
}

@misc{debeziumcommunityDebezium,
  title = {Debezium},
  author = {{Debezium Community}},
  journal = {Debezium},
  abstract = {Debezium is an open source distributed platform for change data capture. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go wrong.},
  howpublished = {https://debezium.io/},
  file = {/home/dimdakis/Zotero/storage/GFF7BNDQ/debezium.io.html}
}

@misc{degioanniPhasesPrometheusAdoption2018,
  title = {3 Phases of {{Prometheus}} Adoption},
  author = {Degioanni, Loris},
  year = {2018},
  month = may,
  journal = {InfoWorld},
  abstract = {How to ensure visibility into your next-generation Kubernetes environment},
  howpublished = {https://www.infoworld.com/article/3275887/3-phases-of-prometheus-adoption.html},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/7BKALCZ8/3-phases-of-prometheus-adoption.html}
}

@misc{DeployingContainerizedAPI,
  title = {Deploying a {{Containerized API}} on {{Kubernetes}} | by Victor Steven | {{Level Up Coding}}},
  howpublished = {https://levelup.gitconnected.com/deploying-dockerized-golang-api-on-kubernetes-with-postgresql-mysql-d190e27ac09f},
  file = {/home/dimdakis/Zotero/storage/PRJC474R/Deploying a Containerized API on Kubernetes  by victor steven  Level Up Coding.html}
}

@article{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  journal = {arXiv:1810.04805 [cs]},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/dimdakis/Zotero/storage/NZEG4YAC/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@misc{dhamiUnderstandingBERTWord2020,
  title = {Understanding {{BERT}} \textemdash{} {{Word Embeddings}}},
  author = {Dhami, Dharti},
  year = {2020},
  month = jul,
  journal = {Medium},
  abstract = {BERT Input},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/3GRNVALE/understanding-bert-word-embeddings-7dc4d2ea54ca.html}
}

@misc{DockercomposeCreateDb,
  title = {Docker-Compose and Create Db in {{Postgres}} on Init},
  journal = {Stack Overflow},
  howpublished = {https://stackoverflow.com/questions/59715622/docker-compose-and-create-db-in-postgres-on-init},
  file = {/home/dimdakis/Zotero/storage/FYAYL49B/docker-compose-and-create-db-in-postgres-on-init.html}
}

@misc{DockerimagesPostgres13,
  title = {Docker-Images/Postgres/13 at Main {$\cdot$} Debezium/Docker-Images},
  journal = {GitHub},
  abstract = {Docker images for Debezium. Please log issues in our JIRA at https://issues.redhat.com/projects/DBZ/summary - docker-images/postgres/13 at main {$\cdot$} debezium/docker-images},
  howpublished = {https://github.com/debezium/docker-images},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/FCR52T2E/13.html}
}

@misc{DockerimagesPostgresMain,
  title = {Docker-Images/Postgres/9.6 at Main {$\cdot$} Debezium/Docker-Images},
  journal = {GitHub},
  abstract = {Docker images for Debezium. Please log issues in our JIRA at https://issues.redhat.com/projects/DBZ/summary - docker-images/postgres/9.6 at main {$\cdot$} debezium/docker-images},
  howpublished = {https://github.com/debezium/docker-images},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/VS2MZTN9/9.html}
}

@article{dongUnifiedLanguageModel2019,
  title = {Unified {{Language Model Pre-training}} for {{Natural Language Understanding}} and {{Generation}}},
  author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  year = {2019},
  month = oct,
  journal = {arXiv:1905.03197 [cs]},
  eprint = {1905.03197},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper presents a new UNIfied pre-trained Language Model (UNILM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UNILM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UNILM achieves new state-ofthe-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/dimdakis/Zotero/storage/I3255268/Dong et al. - 2019 - Unified Language Model Pre-training for Natural La.pdf}
}

@misc{dontlooAnswerWhatExactly2019,
  title = {Answer to "{{What}} Exactly Are Keys, Queries, and Values in Attention Mechanisms?"},
  shorttitle = {Answer to "{{What}} Exactly Are Keys, Queries, and Values in Attention Mechanisms?},
  author = {{dontloo}},
  year = {2019},
  month = aug,
  journal = {Cross Validated},
  file = {/home/dimdakis/Zotero/storage/M8ERYT5V/what-exactly-are-keys-queries-and-values-in-attention-mechanisms.html}
}

@misc{doshiTransformersExplainedVisually2021,
  title = {Transformers {{Explained Visually}} ({{Part}} 1): {{Overview}} of {{Functionality}}},
  shorttitle = {Transformers {{Explained Visually}} ({{Part}} 1)},
  author = {Doshi, Ketan},
  year = {2021},
  month = jun,
  journal = {Medium},
  abstract = {A Gentle Guide to Transformers for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.},
  howpublished = {https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/S75ZMAJQ/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452.html}
}

@misc{doshiTransformersExplainedVisually2021a,
  title = {Transformers {{Explained Visually}} ({{Part}} 3): {{Multi-head Attention}}, Deep Dive},
  shorttitle = {Transformers {{Explained Visually}} ({{Part}} 3)},
  author = {Doshi, Ketan},
  year = {2021},
  month = jun,
  journal = {Medium},
  abstract = {A Gentle Guide to the inner workings of Self-Attention, Encoder-Decoder Attention, Attention Score and Masking, in Plain English.},
  howpublished = {https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/SB5AYIX5/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853.html}
}

@misc{doshiTransformersExplainedVisually2021b,
  title = {Transformers {{Explained Visually}} \textemdash{} {{Not}} Just How, but {{Why}} They Work so Well},
  author = {Doshi, Ketan},
  year = {2021},
  month = jun,
  journal = {Medium},
  abstract = {A Gentle Guide to how the Attention Score calculations capture relationships between words in a sequence, in Plain English.},
  howpublished = {https://towardsdatascience.com/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/GZABTYYF/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3.html}
}

@misc{doshiTransformersExplainedVisually2021c,
  title = {Transformers {{Explained Visually}} ({{Part}} 2): {{How}} It Works, Step-by-Step},
  shorttitle = {Transformers {{Explained Visually}} ({{Part}} 2)},
  author = {Doshi, Ketan},
  year = {2021},
  month = jun,
  journal = {Medium},
  abstract = {A Gentle Guide to the Transformer under the hood, and its end-to-end operation.},
  howpublished = {https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/LIZWCDY9/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34.html}
}

@inproceedings{duttaAnnotationSoftwareImages2019,
  title = {The {{VIA Annotation Software}} for {{Images}}, {{Audio}} and {{Video}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Multimedia}}},
  author = {Dutta, Abhishek and Zisserman, Andrew},
  year = {2019},
  month = oct,
  pages = {2276--2279},
  publisher = {{ACM}},
  address = {{Nice France}},
  doi = {10.1145/3343031.3350535},
  isbn = {978-1-4503-6889-6},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/68DG5Y4E/Dutta and Zisserman - 2019 - The VIA Annotation Software for Images, Audio and .pdf}
}

@misc{EasyUseText,
  title = {Easy to {{Use Text Annotation Tool}} | {{Upload}} Documents, Start Annotating, and Create Advanced {{NLP}} Model in a Few Hours.},
  abstract = {Easy to Use Text Annotation Tool | Upload documents in native PDF, CSV, Docx, html or ZIP format, start annotating, and create advanced NLP model in a few hours. Collaborate with other users to accelerate the document annotation process. Manage users, assign documents and track the annotation progress.        UBIAI high quality OCR annotation allow you to label native PDFs and images directly and auto annotate your dataset in multiple language annotation such as English, French and Arabic. UBIAI let you auto annotate your dataset using dictionaries (word, sentence and Regex inputs) and ML model.With UBIAI's built-in machine learning model, you can train and deploy your custom model without any code. Our NLP labeling tool supports Amazon Comprehend, JSON, Spacy, IOB, and CoreNLP formats},
  howpublished = {https://ubiai.tools/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/LKYLFYV5/ubiai.tools.html}
}

@misc{EasyUseTexta,
  title = {Easy to {{Use Text Annotation Tool}} | {{Upload}} Documents, Start Annotating, and Create Advanced {{NLP}} Model in a Few Hours.},
  abstract = {Easy to Use Text Annotation Tool | Upload documents in native PDF, CSV, Docx, html or ZIP format, start annotating, and create advanced NLP model in a few hours. Collaborate with other users to accelerate the document annotation process. Manage users, assign documents and track the annotation progress.        UBIAI high quality OCR annotation allow you to label native PDFs and images directly and auto annotate your dataset in multiple language annotation such as English, French and Arabic. UBIAI let you auto annotate your dataset using dictionaries (word, sentence and Regex inputs) and ML model.With UBIAI's built-in machine learning model, you can train and deploy your custom model without any code. Our NLP labeling tool supports Amazon Comprehend, JSON, Spacy, IOB, and CoreNLP formats},
  howpublished = {https://ubiai.tools/blog/article/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications},
  langid = {english}
}

@misc{EasyUseTextb,
  title = {Easy to {{Use Text Annotation Tool}} | {{Upload}} Documents, Start Annotating, and Create Advanced {{NLP}} Model in a Few Hours.},
  abstract = {Easy to Use Text Annotation Tool | Upload documents in native PDF, CSV, Docx, html or ZIP format, start annotating, and create advanced NLP model in a few hours. Collaborate with other users to accelerate the document annotation process. Manage users, assign documents and track the annotation progress.        UBIAI high quality OCR annotation allow you to label native PDFs and images directly and auto annotate your dataset in multiple language annotation such as English, French and Arabic. UBIAI let you auto annotate your dataset using dictionaries (word, sentence and Regex inputs) and ML model.With UBIAI's built-in machine learning model, you can train and deploy your custom model without any code. Our NLP labeling tool supports Amazon Comprehend, JSON, Spacy, IOB, and CoreNLP formats},
  howpublished = {https://ubiai.tools/blog/article/fine-tuning-transformer-model},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/4AZ686BU/fine-tuning-transformer-model.html}
}

@misc{emilAnswerWhatExactly2020,
  title = {Answer to "{{What}} Exactly Are Keys, Queries, and Values in Attention Mechanisms?"},
  shorttitle = {Answer to "{{What}} Exactly Are Keys, Queries, and Values in Attention Mechanisms?},
  author = {Emil},
  year = {2020},
  month = jan,
  journal = {Cross Validated},
  file = {/home/dimdakis/Zotero/storage/NHQPRWYA/what-exactly-are-keys-queries-and-values-in-attention-mechanisms.html}
}

@misc{EngineConfigurationSQLAlchemy,
  title = {Engine {{Configuration}} \textemdash{} {{SQLAlchemy}} 1.4 {{Documentation}}},
  howpublished = {https://docs.sqlalchemy.org/en/14/core/engines.html}
}

@misc{EventdrivenArchitectureMicroservices2020,
  title = {Event-Driven Architecture and Microservices Combination Concerns},
  year = {2020},
  month = jul,
  journal = {IBM Developer},
  abstract = {Complexities of combining EDA and microservices architecture styles to build scalable, distributed, highly available, fault-tolerant, and high-throughput systems.},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/EA34XYSL/eda-and-microservices-architecture-best-practices.html}
}

@misc{FacebookresearchDetectron22022,
  title = {Facebookresearch/Detectron2},
  year = {2022},
  month = apr,
  abstract = {Detectron2 is a platform for object detection, segmentation and other visual recognition tasks.},
  copyright = {Apache-2.0},
  howpublished = {Meta Research}
}

@misc{FasterRCNNExplained2020,
  title = {Faster {{R-CNN Explained}} for {{Object Detection Tasks}}},
  year = {2020},
  month = nov,
  journal = {Paperspace Blog},
  abstract = {We'll fully explain the Faster R-CNN model, and how it performs object detection using region-proposal networks.},
  howpublished = {https://blog.paperspace.com/faster-r-cnn-explained-object-detection/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/334WJY23/faster-r-cnn-explained-object-detection.html}
}

@misc{FeedforwardNeuralNetwork2022,
  title = {Feedforward {{Neural Network}}: {{Its Layers}}, {{Functions}}, and {{Importance}}},
  shorttitle = {Feedforward {{Neural Network}}},
  year = {2022},
  month = jan,
  journal = {Analytics Vidhya},
  abstract = {Feedforward Neural Networks, also known as Deep feedforward Networks or Multi-layer Perceptrons, are the focus of this article.},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/HHG34CSF/feedforward-neural-network-its-layers-functions-and-importance.html}
}

@misc{FinetunePretrainedModel,
  title = {Fine-Tune a Pretrained Model},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/training},
  file = {/home/dimdakis/Zotero/storage/57CS7XEA/training.html}
}

@misc{FUNSD,
  title = {{{FUNSD}}},
  howpublished = {https://guillaumejaume.github.io/FUNSD/},
  file = {/home/dimdakis/Zotero/storage/WWRRAKIR/FUNSD.html}
}

@misc{GentleIntroductionBatch,
  title = {A {{Gentle Introduction}} to {{Batch Normalization}} for {{Deep Neural Networks}}},
  howpublished = {https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/}
}

@misc{ghoshExtendingFlaskTutorial2021,
  title = {Extending the {{Flask}} Tutorial App into a Fully-Fledged, Highly Available, Cloud-Based Application},
  author = {Ghosh, Ren{\'e}},
  year = {2021},
  month = mar,
  journal = {Medium},
  abstract = {If you're starting out in Flask development, you'll likely have followed the online tutorial over at pallet projects\ldots},
  langid = {english}
}

@misc{GOCDCPostgres,
  title = {{{GOCDC}} and {{Postgres}}},
  howpublished = {https://dev.to/thiagosilvaf/gocdc-and-postgres-1m4m}
}

@misc{GorillaMux2021,
  title = {Gorilla/Mux},
  year = {2021},
  month = oct,
  abstract = {A powerful HTTP router and URL matcher for building Go web servers with ü¶ç},
  copyright = {BSD-3-Clause},
  howpublished = {Gorilla Web Toolkit},
  keywords = {go,gorilla,http,middleware,mux,router}
}

@misc{GPT3PowersNext2021,
  title = {{{GPT-3 Powers}} the {{Next Generation}} of {{Apps}}},
  year = {2021},
  month = mar,
  journal = {OpenAI},
  abstract = {Over 300 applications are delivering GPT-3\textendash powered search, conversation, text completion, and other advanced AI features through our API.},
  howpublished = {https://openai.com/blog/gpt-3-apps/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/MB95T8TI/gpt-3-apps.html}
}

@misc{GrafanaOpenObservability,
  title = {Grafana: {{The}} Open Observability Platform},
  shorttitle = {Grafana},
  journal = {Grafana Labs},
  abstract = {Grafana is the open source analytics \& monitoring solution for every database.},
  howpublished = {https://grafana.com/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/GTHDN47S/grafana.com.html}
}

@misc{guptaDeepLearningFeedforward2018,
  title = {Deep {{Learning}}: {{Feedforward Neural Network}}},
  shorttitle = {Deep {{Learning}}},
  author = {Gupta, Tushar},
  year = {2018},
  month = dec,
  journal = {Medium},
  abstract = {Coming to the third part of the series. In this article I would be explain the concept of Deep Feedforward Networks.},
  howpublished = {https://towardsdatascience.com/deep-learning-feedforward-neural-network-26a6705dbdc7},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/9E8W4RAA/deep-learning-feedforward-neural-network-26a6705dbdc7.html}
}

@article{HarmonicMean2022,
  title = {Harmonic Mean},
  year = {2022},
  month = apr,
  journal = {Wikipedia},
  abstract = {In mathematics, the harmonic mean is one of several kinds of average, and in particular, one of the Pythagorean means. It is sometimes appropriate for situations when the average rate is desired. The harmonic mean can be expressed as the reciprocal of the arithmetic mean of the reciprocals of the given set of observations. As a simple example, the harmonic mean of 1, 4, and 4 is                                                (                                                                                     1                                            -                       1                                                           +                                        4                                            -                       1                                                           +                                        4                                            -                       1                                                                          3                                         )                                   -             1                             =                                 3                                                                1                   1                                               +                                                   1                   4                                               +                                                   1                   4                                                                          =                                 3             1.5                             =         2                  .                 \{\textbackslash displaystyle \textbackslash left(\{\textbackslash frac \{1\^\{-1\}+4\^\{-1\}+4\^\{-1\}\}\{3\}\}\textbackslash right)\^\{-1\}=\{\textbackslash frac \{3\}\{\{\textbackslash frac \{1\}\{1\}\}+\{\textbackslash frac \{1\}\{4\}\}+\{\textbackslash frac \{1\}\{4\}\}\}\}=\{\textbackslash frac \{3\}\{1.5\}\}=2\textbackslash,.\}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1083612353},
  file = {/home/dimdakis/Zotero/storage/RC9926P8/Harmonic_mean.html}
}

@misc{Helm,
  title = {Helm},
  abstract = {Helm - The Kubernetes Package Manager.},
  howpublished = {https://helm.sh/},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/LQN9LERF/helm.sh.html}
}

@misc{horevBERTExplainedState2018,
  title = {{{BERT Explained}}: {{State}} of the Art Language Model for {{NLP}}},
  shorttitle = {{{BERT Explained}}},
  author = {Horev, Rani},
  year = {2018},
  month = nov,
  journal = {Medium},
  abstract = {An approachable and understandable explanation of BERT, a recent paper by Google that achieved SOTA results in wide variety of NLP tasks.},
  howpublished = {https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/4Y72R99R/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270.html}
}

@misc{HowGiantCompanies,
  title = {How 8 {{Giant Companies Use Kubernetes}} \& 60 {{Others That Use It}}},
  abstract = {This guide gives an in-depth look at how 8 well-known companies use Kubernetes \& lists 60 others that use it.},
  howpublished = {https://www.containiq.com/post/companies-using-kubernetes},
  file = {/home/dimdakis/Zotero/storage/F2A44NEI/companies-using-kubernetes.html}
}

@misc{HowIntegrateKafka,
  title = {How to Integrate {{Kafka}} with {{Istio}} on {{OpenShift}}},
  howpublished = {https://labs.consol.de/development/2021/02/02/istio\_and\_kafka\_on\_openshift.html}
}

@misc{HowPlotTrain,
  title = {How to Plot Train and Validation Accuracy Graph?},
  howpublished = {https://discuss.pytorch.org/t/how-to-plot-train-and-validation-accuracy-graph/105524}
}

@misc{HowUseChange,
  title = {How to Use {{Change Data Capture}} ({{CDC}}) with {{Postgres}}},
  howpublished = {https://dev.to/thiagosilvaf/how-to-use-change-database-capture-cdc-in-postgres-37b8}
}

@misc{HuggingFaceAI,
  title = {Hugging {{Face}} \textendash{} {{The AI}} Community Building the Future.},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/},
  file = {/home/dimdakis/Zotero/storage/RG3K8PCF/huggingface.co.html}
}

@misc{HuggingFaceAIa,
  title = {Hugging {{Face}} \textendash{} {{The AI}} Community Building the Future.},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/datasets},
  file = {/home/dimdakis/Zotero/storage/TZ5SNMYA/datasets.html}
}

@misc{HuggingFaceTransformers,
  title = {Hugging {{Face Transformers Package}} - {{What Is It}} and {{How To Use It}}},
  journal = {KDnuggets},
  abstract = {The rapid development of Transformers have brought a new wave of powerful tools to natural language processing. These models are large and very expensive to train, so pre-trained versions are shared and leveraged by researchers and practitioners. Hugging Face offers a wide variety of pre-trained transformers as open-source libraries, and\ldots},
  chapter = {2021 Feb Tutorials, Overviews},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/WR3ETSKR/hugging-face-transformer-basics.html}
}

@misc{InstallationGuideNGINX,
  title = {Installation {{Guide}} - {{NGINX Ingress Controller}}},
  howpublished = {https://kubernetes.github.io/ingress-nginx/deploy/}
}

@misc{IntelligentDocumentProcessing,
  title = {Intelligent Document Processing with {{AI}}},
  howpublished = {https://nanonets.com/blog/receipt-ocr/\%23receipt-digitization-using-tesseract}
}

@misc{issaevBeginnerGuideLoading2020,
  title = {Beginner's {{Guide}} to {{Loading Image Data}} with {{PyTorch}}},
  author = {Issaev, Sergei},
  year = {2020},
  month = dec,
  journal = {Medium},
  abstract = {As data scientists, we deal with incoming data in a wide variety of formats. When it comes to loading image data with PyTorch, the\ldots},
  howpublished = {https://towardsdatascience.com/beginners-guide-to-loading-image-data-with-pytorch-289c60b7afec},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/R3BZHR8S/beginners-guide-to-loading-image-data-with-pytorch-289c60b7afec.html}
}

@misc{KafkaConnectConfluent,
  title = {Kafka {{Connect}} | {{Confluent Documentation}}},
  howpublished = {https://docs.confluent.io/platform/current/connect/index.html},
  file = {/home/dimdakis/Zotero/storage/S7T2ZSPI/index.html}
}

@misc{kamaniImplementingKafkaProducer,
  title = {Implementing a {{Kafka Producer}} and {{Consumer In Golang}} ({{With Full Examples}}) {{For Production}}},
  author = {Kamani, Soham},
  howpublished = {https://www.sohamkamani.com/golang/working-with-kafka/}
}

@misc{khannaWordPieceSubwordbasedTokenization2021,
  title = {{{WordPiece}}: {{Subword-based}} Tokenization Algorithm},
  shorttitle = {{{WordPiece}}},
  author = {Khanna, Chetna},
  year = {2021},
  month = aug,
  journal = {Medium},
  abstract = {Understand subword-based tokenization algorithm used by state-of-the-art NLP models{$\mkern1mu$}\textemdash{$\mkern1mu$}WordPiece},
  howpublished = {https://towardsdatascience.com/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/BKBEV3K5/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7.html}
}

@misc{korstanjeF1Score2021,
  title = {The {{F1}} Score},
  author = {Korstanje, Joos},
  year = {2021},
  month = aug,
  journal = {Medium},
  abstract = {All you need to know about the F1 score in machine learning. With an example applying the F1 score in Python.},
  howpublished = {https://towardsdatascience.com/the-f1-score-bec2bbc38aa6},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/JZC27VAH/the-f1-score-bec2bbc38aa6.html}
}

@misc{KubernetesAdoptionTrends,
  title = {Kubernetes {{Adoption Trends Report}}},
  howpublished = {https://www.cockroachlabs.com/guides/kubernetes-trends/},
  file = {/home/dimdakis/Zotero/storage/5X8X75AZ/kubernetes-trends.html}
}

@misc{KubernetesOperatorStateful2021,
  title = {Kubernetes {{Operator}} | {{Stateful Kubernetes Application}}},
  year = {2021},
  month = may,
  journal = {Cloud Training Program},
  abstract = {Kubernetes Operator are a replacement of the engineers who are responsible to operate the system. It has knowledge about the current and the desired state.},
  howpublished = {https://k21academy.com/docker-kubernetes/kubernetes-operator/},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/ZRC7M3VZ/kubernetes-operator.html}
}

@misc{KubernetesSecurityBest,
  title = {Kubernetes {{Security Best Practices}}: 10 {{Steps}} to {{Securing K8s}}},
  howpublished = {https://www.aquasec.com/cloud-native-academy/kubernetes-in-production/kubernetes-security-best-practices-10-steps-to-securing-k8s/}
}

@misc{kudariSQLAlchemyPythonTutorial2020,
  title = {{{SQLAlchemy}} \textemdash{} {{Python Tutorial}}},
  author = {Kudari, Vinay},
  year = {2020},
  month = sep,
  journal = {Medium},
  abstract = {We often encounter data as Relational Databases. To work with them we generally would need to write raw SQL queries, pass them to the\ldots},
  howpublished = {https://towardsdatascience.com/sqlalchemy-python-tutorial-79a577141a91},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/AG7KGNPD/sqlalchemy-python-tutorial-79a577141a91.html}
}

@misc{kutayChangeDataCapture2021,
  title = {Change {{Data Capture}} ({{CDC}}): {{What}} It Is and {{How}} It {{Works}}},
  shorttitle = {Change {{Data Capture}} ({{CDC}})},
  author = {Kutay, John},
  year = {2021},
  month = may,
  journal = {Striim},
  abstract = {Change Data Capture is ideal for real time data movement. Learn how it works, the best use cases for CDC, and the role it plays in streaming ETL.},
  chapter = {CDC},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/7MYRK8MN/change-data-capture-cdc-what-it-is-and-how-it-works.html}
}

@misc{LabelStudioOpen,
  title = {Label {{Studio}} \textendash{} {{Open Source Data Labeling}}},
  abstract = {Most flexible data labeling tool that supports all your data types. Prepare training data for computer vision, natural language processing, speech, voice, and video models.},
  howpublished = {https://labelstud.io/},
  file = {/home/dimdakis/Zotero/storage/VJIMR2Y7/labelstud.io.html}
}

@misc{LAMBERT2022,
  title = {{{LAMBERT}}},
  year = {2022},
  month = apr,
  abstract = {Publicly released code for the LAMBERT model},
  howpublished = {Applica}
}

@misc{LayoutLMExplained2022,
  title = {{{LayoutLM Explained}}},
  year = {2022},
  month = mar,
  journal = {AI \& Machine Learning Blog},
  abstract = {LayoutLM is a deep learning model used to perform document processing. In this article we share a LayoutLM tutorial, a deeper dive in architecture, and provide code samples for HuggingFace LayoutLM},
  howpublished = {https://nanonets.com/blog/layoutlm-explained/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/46ISG8W8/layoutlm-explained.html}
}

@misc{LayoutLMV2,
  title = {{{LayoutLMV2}}},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/model\_doc/layoutlmv2},
  file = {/home/dimdakis/Zotero/storage/F969IVJA/layoutlmv2.html}
}

@misc{leeConfusionMatrixPrecision2021,
  title = {Confusion {{Matrix}} ({{Precision}}, {{Recall}}, {{F1 Score}})},
  author = {Lee, Ron},
  year = {2021},
  month = mar,
  journal = {Medium},
  abstract = {The confusion matrix is a useful tools to measure the effectiveness of model. It can be explained in a table with 4 different combination\ldots},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/LDSVARRT/confusion-matrix-precision-recall-f1-score-76e579eb2d48.html}
}

@misc{LessonsLearnedRunning,
  title = {Lessons {{Learned}} from {{Running Debezium}} with {{PostgreSQL}} on {{Amazon RDS}}},
  howpublished = {https://debezium.io/blog/2020/02/25/lessons-learned-running-debezium-with-postgresql-on-rds/}
}

@misc{LightTagTextAnnotation,
  title = {{{LightTag}} - {{The Text Annotation Tool For Teams}}},
  howpublished = {https://www.lighttag.io/},
  file = {/home/dimdakis/Zotero/storage/L6CDSPXX/www.lighttag.io.html}
}

@misc{LogicalDecodingOutput,
  title = {Logical {{Decoding Output Plug-in Installation}} for {{PostgreSQL}} :: {{Debezium Documentation}}},
  howpublished = {https://debezium.io/documentation/reference/postgres-plugins.html},
  file = {/home/dimdakis/Zotero/storage/AP3XHUXI/postgres-plugins.html}
}

@misc{maoLayerNormalizationExplained2019,
  title = {Layer {{Normalization Explained}}},
  author = {Mao, Lei},
  year = {2019},
  month = may,
  journal = {Lei Mao's Log Book},
  abstract = {Layer Normalization vs Batch Normalization vs Instance Normalization},
  howpublished = {https://leimao.github.io/blog/Layer-Normalization/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/7UDE63SQ/Layer-Normalization.html}
}

@misc{MethodStrucTexTTask,
  title = {Method: {{StrucTexT}} - {{Task}} 3 - {{Key Information Extraction}} - {{ICDAR}} 2019 {{Robust Reading Challenge}} on {{Scanned Receipts OCR}} and {{Information Extraction}} - {{Robust Reading Competition}}},
  howpublished = {https://rrc.cvc.uab.es/?ch=13\&com=evaluation\&view=method\_info\&task=3\&m=90335},
  file = {/home/dimdakis/Zotero/storage/TFEHEXIZ/rrc.cvc.uab.es.html}
}

@misc{MicrosoftNvidiaTeam2021,
  title = {Microsoft and {{Nvidia}} Team up to Train One of the World's Largest Language Models},
  year = {2021},
  month = oct,
  journal = {VentureBeat},
  abstract = {Microsoft and Nvidia claim to have trained one of the world's largest natural language models, containing 530 billion parameters.},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/XFVTZ7CQ/microsoft-and-nvidia-team-up-to-train-one-of-the-worlds-largest-language-models.html}
}

@misc{MinikubeStart,
  title = {Minikube Start},
  journal = {minikube},
  abstract = {minikube is local Kubernetes},
  howpublished = {https://minikube.sigs.k8s.io/docs/start/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/G46IY2P3/start.html}
}

@misc{ModelsHuggingFace,
  title = {Models - {{Hugging Face}}},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/models},
  file = {/home/dimdakis/Zotero/storage/383T6HMK/models.html}
}

@misc{mohajonConfusionMatrixYour2021,
  title = {Confusion {{Matrix}} for {{Your Multi-Class Machine Learning Model}}},
  author = {Mohajon, Joydwip},
  year = {2021},
  month = jul,
  journal = {Medium},
  abstract = {A beginner's guide on how to calculate Precision, Recall, F1-score for a multi-class classification problem.},
  howpublished = {https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/7DHL6MFR/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826.html}
}

@misc{montantesBERTTransformersHow2021,
  title = {{{BERT Transformers}} \textemdash{} {{How Do They Work}}?},
  author = {Montantes, James},
  year = {2021},
  month = apr,
  journal = {Medium},
  abstract = {BERT Transformers Are Revolutionary But How Do They Work?},
  howpublished = {https://becominghuman.ai/bert-transformers-how-do-they-work-cd44e8e31359},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/L6JTA7WT/bert-transformers-how-do-they-work-cd44e8e31359.html}
}

@misc{munozAttentionAllYou2021,
  title = {Attention Is All You Need: {{Discovering}} the {{Transformer}} Paper},
  shorttitle = {Attention Is All You Need},
  author = {Mu{\~n}oz, Eduardo},
  year = {2021},
  month = feb,
  journal = {Medium},
  abstract = {Detailed implementation of a Transformer model in Tensorflow},
  howpublished = {https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/W7IF9JUN/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634.html}
}

@misc{naincyjainEffectBatchSize,
  title = {Effect of {{Batch Size}} on {{Training Process}} and Results by {{Gradient Accumulation}}},
  author = {Naincyjain},
  howpublished = {https://medium.com/analytics-vidhya/effect-of-batch-size-on-training-process-and-results-by-gradient-accumulation-e7252ee2cb3f}
}

@misc{nathGracefulShutdownGolang2019,
  title = {Graceful Shutdown of {{Golang}} Servers Using {{Context}} and {{OS}} Signals},
  author = {Nath, Pinku Deb},
  year = {2019},
  month = may,
  journal = {Medium},
  abstract = {Whenever a server needs to shut down for various reasons, the common one being OS interrupts, we would want our servers to shut down\ldots},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/6WUCTKFQ/graceful-shutdown-of-golang-servers-using-context-and-os-signals-cc1fa2c55e97.html}
}

@misc{OpenAIAPI,
  title = {{{OpenAI API}}},
  abstract = {An API for accessing new AI models developed by OpenAI},
  howpublished = {https://beta.openai.com},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/9BIUKY5J/examples.html}
}

@misc{OperatorPattern,
  title = {Operator Pattern},
  journal = {Kubernetes},
  abstract = {Operators are software extensions to Kubernetes that make use of custom resources to manage applications and their components. Operators follow Kubernetes principles, notably the control loop. Motivation The Operator pattern aims to capture the key aim of a human operator who is managing a service or set of services. Human operators who look after specific applications and services have deep knowledge of how the system ought to behave, how to deploy it, and how to react if there are problems.},
  chapter = {docs},
  howpublished = {https://kubernetes.io/docs/concepts/extend-kubernetes/operator/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/2B974M26/operator.html}
}

@misc{OverviewICDAR2019,
  title = {Overview - {{ICDAR}} 2019 {{Robust Reading Challenge}} on {{Scanned Receipts OCR}} and {{Information Extraction}} - {{Robust Reading Competition}}},
  howpublished = {https://rrc.cvc.uab.es/?ch=13},
  file = {/home/dimdakis/Zotero/storage/PTVVKC7T/rrc.cvc.uab.es.html}
}

@misc{PartHandlingMigrations,
  title = {Part 5: {{Handling Migrations With Gorm}} in {{Go}} | {{Code}} Sahara},
  howpublished = {https://codesahara.com/blog/making-migrations-with-gorm-in-go/},
  keywords = {gorm migrations},
  file = {/home/dimdakis/Zotero/storage/YEJ2YA5T/making-migrations-with-gorm-in-go.html}
}

@misc{PGOPostgresOperator,
  title = {{{PGO}}, the {{Postgres Operator}} from {{Crunchy Data}}},
  howpublished = {https://access.crunchydata.com/documentation/postgres-operator/5.0.5/tutorial/connect-cluster/},
  file = {/home/dimdakis/Zotero/storage/KYAPNFLH/connect-cluster.html}
}

@misc{PodKubernetesEngine,
  title = {Pod ~|~ {{Kubernetes Engine Documentation}} ~|~ {{Google Cloud}}},
  howpublished = {https://cloud.google.com/kubernetes-engine/docs/concepts/pod},
  file = {/home/dimdakis/Zotero/storage/MKUX9LLS/pod.html}
}

@misc{PostgreSQLLinuxDownloads,
  title = {{{PostgreSQL}}: {{Linux}} Downloads ({{Ubuntu}})},
  howpublished = {https://www.postgresql.org/download/linux/ubuntu/},
  file = {/home/dimdakis/Zotero/storage/UZ6R4S43/ubuntu.html}
}

@misc{ProductionGradeContainerOrchestration,
  title = {Production-{{Grade Container Orchestration}}},
  journal = {Kubernetes},
  abstract = {Production-Grade Container Orchestration},
  howpublished = {https://kubernetes.io/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/HVIGLI4Z/kubernetes.io.html}
}

@misc{prometheusPrometheusMonitoringSystema,
  title = {Prometheus - {{Monitoring}} System \& Time Series Database},
  author = {Prometheus},
  abstract = {An open-source monitoring system with a dimensional data model, flexible query language, efficient time series database and modern alerting approach.},
  howpublished = {https://prometheus.io/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/ADG34MBU/prometheus.io.html}
}

@misc{pythonFlaskExampleSetting,
  title = {Flask by {{Example}} \textendash{} {{Setting}} up {{Postgres}}, {{SQLAlchemy}}, and {{Alembic}} \textendash{} {{Real Python}}},
  author = {Python, Real},
  abstract = {This tutorial shows you how to process text and then setup a task queue with Flask. In part two, we'll set up our PostgreSQL database along with SQLAlchemy and Alembic to handle migrations.},
  howpublished = {https://realpython.com/flask-by-example-part-2-postgres-sqlalchemy-and-alembic/},
  langid = {english}
}

@misc{pythonHowBuildCommand,
  title = {How to {{Build Command Line Interfaces}} in {{Python With}} Argparse \textendash{} {{Real Python}}},
  author = {Python, Real},
  abstract = {In this step-by-step Python tutorial, you'll learn how to take your command line Python scripts to the next level by adding a convenient command line interface that you can write with argparse.},
  howpublished = {https://realpython.com/command-line-interfaces-python-argparse/},
  langid = {english}
}

@misc{PythonHTTPRequest2019,
  title = {Python {{HTTP Request Tutorial}}: {{Get}} \& {{Post HTTP}} \& {{JSON Requests}}},
  shorttitle = {Python {{HTTP Request Tutorial}}},
  year = {2019},
  month = sep,
  journal = {DataCamp Community},
  abstract = {Learn about Python Request library and how to make a request. Follow examples to Get \& Post HTTP \& JSON requests today!},
  howpublished = {https://www.datacamp.com/community/tutorials/making-http-requests-in-python},
  file = {/home/dimdakis/Zotero/storage/S6GWPILF/making-http-requests-in-python.html}
}

@misc{rahmanCOMPASCaseStudy2020,
  title = {{{COMPAS Case Study}}: {{Fairness}} of a {{Machine Learning Model}}},
  shorttitle = {{{COMPAS Case Study}}},
  author = {Rahman, Farhan},
  year = {2020},
  month = sep,
  journal = {Medium},
  abstract = {COMPAS and classifier fairness},
  howpublished = {https://towardsdatascience.com/compas-case-study-fairness-of-a-machine-learning-model-f0f804108751},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/XREAJN7D/compas-case-study-fairness-of-a-machine-learning-model-f0f804108751.html}
}

@misc{ReleasesStrimziStrimzikafkaoperator,
  title = {Releases {$\cdot$} Strimzi/Strimzi-Kafka-Operator},
  journal = {GitHub},
  abstract = {Apache Kafka running on Kubernetes. Contribute to strimzi/strimzi-kafka-operator development by creating an account on GitHub.},
  howpublished = {https://github.com/strimzi/strimzi-kafka-operator/releases},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/GXFZJ9FN/0.26.html}
}

@misc{ResultsICDAR2019,
  title = {Results - {{ICDAR}} 2019 {{Robust Reading Challenge}} on {{Scanned Receipts OCR}} and {{Information Extraction}} - {{Robust Reading Competition}}},
  howpublished = {https://rrc.cvc.uab.es/?ch=13\&com=evaluation\&task=3},
  file = {/home/dimdakis/Zotero/storage/JSHQ9GUX/rrc.cvc.uab.es.html}
}

@misc{RoBERTa,
  title = {{{RoBERTa}}},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/model\_doc/roberta},
  file = {/home/dimdakis/Zotero/storage/DRSGK8HP/roberta.html}
}

@misc{RoleThatSMEs,
  title = {The {{Role That SMEs Will Play}} in {{Rebuilding}} the {{Irish Economy}} ({{Free Downloadable Executive Summary}})},
  abstract = {The Role That SMEs Will Play in Rebuilding the Irish Economy.},
  howpublished = {https://aibf.ie/times/the-role-that-smes-will-play-in-rebuilding-the-irish-economy-free-downloadable-executive-summary/}
}

@misc{rusevImboxPythonIMAP,
  title = {Imbox: {{Python IMAP}} for {{Human}} Beings},
  shorttitle = {Imbox},
  author = {Rusev, Martin},
  copyright = {MIT},
  keywords = {email;,IMAP;,parsing emails},
  file = {/home/dimdakis/Zotero/storage/T43ZM4WU/imbox.html}
}

@misc{saysKubernetesDesiredState2019,
  title = {Kubernetes - {{Desired State}} and {{Control Loops}}},
  author = {{says}, Sub},
  year = {2019},
  month = sep,
  journal = {The IT Hollow},
  abstract = {If you've just gotten started with Kubernetes, you might be curious to know how the desired state is achieved? Think\ldots},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/BY28S9ZV/kubernetes-desired-state-and-control-loops.html}
}

@misc{seanWhatExactlyAre2020,
  type = {Forum Post},
  title = {What Exactly Are Keys, Queries, and Values in Attention Mechanisms?},
  author = {Sean},
  year = {2020},
  month = dec,
  journal = {Cross Validated},
  file = {/home/dimdakis/Zotero/storage/4K98HYZS/what-exactly-are-keys-queries-and-values-in-attention-mechanisms.html}
}

@misc{Secrets,
  title = {Secrets},
  journal = {Kubernetes},
  abstract = {A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in a container image. Using a Secret means that you don't need to include confidential data in your application code. Because Secrets can be created independently of the Pods that use them, there is less risk of the Secret (and its data) being exposed during the workflow of creating, viewing, and editing Pods.},
  chapter = {docs},
  howpublished = {https://kubernetes.io/docs/concepts/configuration/secret/},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/NGK4PZM8/secret.html}
}

@misc{sotiriouFaultToleranceKubernetes2020,
  title = {Fault {{Tolerance}} in {{Kubernetes Clusters}}},
  author = {Sotiriou, Christos},
  year = {2020},
  month = apr,
  journal = {The Startup},
  abstract = {What is it, why you should care, and how to apply it using Quarkus},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/M8L585U5/fault-tolerance-in-kubernetes-clusters-f5d707bc8b5c.html}
}

@misc{SQLAlchemyDatabaseToolkit,
  title = {{{SQLAlchemy}} - {{The Database Toolkit}} for {{Python}}},
  howpublished = {https://www.sqlalchemy.org/},
  file = {/home/dimdakis/Zotero/storage/LKSWKVUF/www.sqlalchemy.org.html}
}

@misc{SquareSolutionsTools,
  title = {{Square: Solutions \& Tools to Grow Your Business}},
  shorttitle = {{Square}},
  journal = {Square},
  abstract = {Square helps millions of sellers run their business \textendash{} from secure credit card processing, point of sale solutions to setting up a free online store. Get paid faster with Square and sign up today!},
  howpublished = {https://squareup.com/ie/en},
  langid = {en-IE},
  file = {/home/dimdakis/Zotero/storage/YH6H72QL/en.html}
}

@misc{StrimziDocumentation16,
  title = {Strimzi {{Documentation}} (0.16.2)},
  howpublished = {https://strimzi.io/docs/0.16.2/},
  file = {/home/dimdakis/Zotero/storage/2I5KXDBZ/0.16.2.html}
}

@misc{StrimziQuickStart,
  title = {Strimzi {{Quick Start}} Guide (0.26.0)},
  howpublished = {https://strimzi.io/docs/operators/latest/quickstart.html},
  file = {/home/dimdakis/Zotero/storage/TB2Y3BYQ/quickstart.html}
}

@misc{SummaryTokenizers,
  title = {Summary of the Tokenizers},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/tokenizer\_summary},
  file = {/home/dimdakis/Zotero/storage/AJXUULVC/tokenizer_summary.html}
}

@misc{syalHuggingFaceStep2020,
  title = {Hugging {{Face}}: {{A Step Towards Democratizing NLP}}},
  shorttitle = {Hugging {{Face}}},
  author = {Syal, Anuj},
  year = {2020},
  month = dec,
  journal = {Medium},
  abstract = {It's not an emoji, it's NLP for everyone},
  howpublished = {https://towardsdatascience.com/hugging-face-a-step-towards-democratizing-nlp-2c79f258c951},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/4T4QQWG6/hugging-face-a-step-towards-democratizing-nlp-2c79f258c951.html}
}

@misc{TEMxXnwlvmb,
  title = {{{tEMxXnwlvmb}}},
  howpublished = {https://link.medium.com/tEMxXnwlvmb}
}

@misc{thea.i.hacker-michaelphiIllustratedGuideTransformers2020,
  title = {Illustrated {{Guide}} to {{Transformers Neural Network}}: {{A}} Step by Step Explanation},
  shorttitle = {Illustrated {{Guide}} to {{Transformers Neural Network}}},
  author = {{The A.I. Hacker - Michael Phi}},
  year = {2020},
  month = apr,
  abstract = {Transformers are the rage nowadays, but how do they work? This video demystifies the novel neural network architecture with step by step explanation and illustrations on how transformers work. CORRECTIONS: The sine and cosine functions are actually applied to the embedding dimensions and time steps!  Audo Studio | Automagically Make Audio Recordings Studio Quality https://www.audostudio.com/ Magic Mic | Join waitlist and get it FREE forever when launched! üéôÔ∏è https://magicmic.ai/ Audo AI | Audio Background Noise Removal Developer API and SDK https://audo.ai/ Subscribe to my email newsletter for updated Content.  No spam üôÖ‚Äç\male Ô∏è only gold ü•á. https://bit.ly/320hUdx Discord Server: Join a community of A.I. Hackers https://discord.gg/9wSTT4F Hugging Face Write with Transformers https://transformer.huggingface.co/}
}

@misc{TipsTricksRunning,
  title = {Tips \& {{Tricks}} for Running {{Strimzi}} with Kubectl},
  howpublished = {https://strimzi.io/blog/2020/07/22/tips-and-tricks-for-running-strimzi-with-kubectl/},
  file = {/home/dimdakis/Zotero/storage/I9CTCBJW/tips-and-tricks-for-running-strimzi-with-kubectl.html}
}

@misc{TokenClassification,
  title = {Token Classification},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/main/en/tasks/token\_classification},
  file = {/home/dimdakis/Zotero/storage/XMZ6VTA3/token_classification.html}
}

@misc{TransformerArchitectureSelfattention,
  title = {Transformer Architecture, Self-Attention},
  abstract = {Explore and run machine learning code with Kaggle Notebooks | Using data from No attached data sources},
  howpublished = {https://www.kaggle.com/residentmario/transformer-architecture-self-attention}
}

@misc{Transformers,
  title = {Transformers},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/index},
  file = {/home/dimdakis/Zotero/storage/BCF2836N/index.html}
}

@misc{tsengAnswerWhatExactly2020,
  title = {Answer to "{{What}} Exactly Are Keys, Queries, and Values in Attention Mechanisms?"},
  shorttitle = {Answer to "{{What}} Exactly Are Keys, Queries, and Values in Attention Mechanisms?},
  author = {Tseng, Sam},
  year = {2020},
  month = apr,
  journal = {Cross Validated},
  file = {/home/dimdakis/Zotero/storage/E8VJSJ23/what-exactly-are-keys-queries-and-values-in-attention-mechanisms.html}
}

@misc{UnderstandingBackpropagationNeural,
  title = {Understanding {{Backpropagation}} in a {{Neural Network}} - 1},
  howpublished = {https://www.linkedin.com/pulse/understanding-backpropagation-neural-network-1-srikanth-machiraju/}
}

@misc{UsingHelm,
  title = {Using {{Helm}}},
  howpublished = {https://helm.sh/docs/intro/using\_helm/}
}

@misc{UsingSQLAlchemyFlask2017,
  title = {Using {{SQLAlchemy}} with {{Flask}} to {{Connect}} to {{PostgreSQL}}},
  year = {2017},
  month = aug,
  journal = {vsupalov.com},
  abstract = {Connecting to PostgreSQL from a Flask app, and defining models.},
  howpublished = {https://vsupalov.com/flask-sqlalchemy-postgres/},
  langid = {english}
}

@misc{UsingSQLAlchemyFlask2020,
  title = {Using {{SQLAlchemy}} with {{Flask}} and {{PostgreSQL}}},
  year = {2020},
  month = jan,
  journal = {Stack Abuse},
  abstract = {Nowadays, Object-Relational Mappers like SQLAlchemy are used as a bridge between applications and SQL databases and make it easy to work with them programmatically.},
  howpublished = {https://stackabuse.com/using-sqlalchemy-with-flask-and-postgresql/},
  langid = {english}
}

@misc{UsingStrimzi,
  title = {Using {{Strimzi}}},
  howpublished = {https://strimzi.io/docs/operators/latest/full/using.html}
}

@article{vaswaniAttentionAllYou,
  title = {Attention Is {{All}} You {{Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  pages = {11},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/NI87Q83X/Vaswani et al. - Attention is All you Need.pdf}
}

@misc{velichkoHowGraspContainers,
  title = {How to {{Grasp Containers}} - {{Efficient Learning Path}} - {{Ivan Velichko}}},
  author = {Velichko, Ivan},
  howpublished = {https://iximiuz.com/en/posts/container-learning-path/}
}

@misc{vijayraniaDifferentNormalizationLayers2021,
  title = {Different {{Normalization Layers}} in {{Deep Learning}}},
  author = {Vijayrania, Nilesh},
  year = {2021},
  month = mar,
  journal = {Medium},
  abstract = {Presently Deep Learning has been revolutionizing many subfields such as natural language processing, computer vision, robotics, etc. Deep\ldots},
  howpublished = {https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/CHIDP3NH/different-normalization-layers-in-deep-learning-1a7214ff71d6.html}
}

@misc{wadaLabelmeImagePolygonal2022,
  title = {Labelme: {{Image Polygonal Annotation}} with {{Python}}},
  shorttitle = {Labelme},
  author = {Wada, Kentaro},
  year = {2022},
  month = apr,
  doi = {10.5281/zenodo.5711226},
  abstract = {Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation).}
}

@misc{WhatAttention,
  title = {What Is {{Attention}}?},
  abstract = {Attention is becoming increasingly popular in machine learning, but what makes it such an attractive concept? What is the relationship [\ldots ]},
  howpublished = {https://machinelearningmastery.com/what-is-attention/}
}

@misc{WhatEventdrivenArchitecture,
  title = {What Is Event-Driven Architecture?},
  howpublished = {https://www.redhat.com/en/topics/integration/what-is-event-driven-architecture},
  file = {/home/dimdakis/Zotero/storage/FR9MBNIR/what-is-event-driven-architecture.html}
}

@misc{WhatHelmChart2019,
  title = {What {{Is A Helm Chart}}? \textendash{} {{A Beginner}}'s {{Guide}}},
  shorttitle = {What {{Is A Helm Chart}}?},
  year = {2019},
  month = aug,
  journal = {Coveros},
  abstract = {Helm is a Kubernetes package and operations manager. The name ``kubernetes'' is derived from the Greek word for ``pilot'' or ``helmsman'', making Helm its steering wheel. Using a packaging manager, Charts, Helm allows us to package Kubernetes releases into a convenient zip (.tgz) file. A Helm chart can contain any number of Kubernetes objects, all [\ldots ]},
  langid = {american},
  file = {/home/dimdakis/Zotero/storage/QIRGPMNE/what-is-a-helm-chart-a-beginners-guide.html}
}

@misc{WhatKubernetesOperator,
  title = {What Is a {{Kubernetes}} Operator?},
  abstract = {A Kubernetes operator is a method of packaging, deploying, and managing an application by extending the functionality of the Kubernetes API.},
  howpublished = {https://www.redhat.com/en/topics/containers/what-is-a-kubernetes-operator},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/IY5I3P4X/what-is-a-kubernetes-operator.html}
}

@misc{WhatMakefileHow,
  title = {What Is a {{Makefile}} and How Does It Work?},
  howpublished = {https://opensource.com/article/18/8/what-how-makefile}
}

@misc{WhatTokenizationTokenization2020,
  title = {What Is {{Tokenization}} | {{Tokenization In NLP}}},
  year = {2020},
  month = may,
  journal = {Analytics Vidhya},
  abstract = {Tokenization is an NLP concept you should know before entering the field. Learn what is tokenization and working of tokenization in NLP using python.},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/W6BWIEGT/what-is-tokenization-nlp.html}
}

@misc{WhatTransferLearning2021,
  title = {What Is Transfer Learning and Why Is It Needed? - {{Beginners}}},
  shorttitle = {What Is Transfer Learning and Why Is It Needed?},
  year = {2021},
  month = mar,
  journal = {Hugging Face Forums},
  abstract = {I am using Hugging Face Models for NLP tasks.  I see a lot of online examples like below which freezes the bottom layers and only train only few top layers. Basic idea is to use transfer learning, without having to train model from scratch. Which makes sense to me.  for layer in model.layers[:-2]:     layer.trainable = False  However i find that accuracy of my model improves significantly when train it from scratch, Are there any downsides to train transformer model from scratch? What is the rig...},
  howpublished = {https://discuss.huggingface.co/t/what-is-transfer-learning-and-why-is-it-needed/4431},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/4JW5Z2JC/4431.html}
}

@misc{WhatZookeeperWhy,
  title = {What Is {{Zookeeper}} and Why Is It Needed for {{Apache Kafka}}? - {{CloudKarafka}}, {{Apache Kafka Message}} Streaming as a {{Service}}},
  shorttitle = {What Is {{Zookeeper}} and Why Is It Needed for {{Apache Kafka}}?},
  abstract = {Are you using Apache Kafka to build message streaming services? Then you might have run into the expression Zookeeper. To us at CloudKarafka, as a Apache Kafka hosting service, it's important that our users understand what Zookeeper is and how it integrates with Kafka.},
  howpublished = {https://www.cloudkarafka.com/blog/cloudkarafka-what-is-zookeeper.html},
  langid = {english},
  file = {/home/dimdakis/Zotero/storage/2BX7YJYB/cloudkarafka-what-is-zookeeper.html}
}

@misc{WhoUsingDebezium,
  title = {Who's {{Using Debezium}}?},
  journal = {Debezium},
  abstract = {Debezium is an open source distributed platform for change data capture. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go wrong.},
  howpublished = {https://debezium.io/community/users/},
  file = {/home/dimdakis/Zotero/storage/HIR6YUTZ/users.html}
}

@article{xuLayoutLMPretrainingText2020,
  title = {{{LayoutLM}}: {{Pre-training}} of {{Text}} and {{Layout}} for {{Document Image Understanding}}},
  shorttitle = {{{LayoutLM}}},
  author = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  year = {2020},
  month = aug,
  journal = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  eprint = {1912.13318},
  eprinttype = {arxiv},
  pages = {1192--1200},
  doi = {10.1145/3394486.3403172},
  abstract = {Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for documentlevel pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/dimdakis/Zotero/storage/GDBIW7J5/Xu et al. - 2020 - LayoutLM Pre-training of Text and Layout for Docu.pdf}
}

@article{xuLayoutLMv2MultimodalPretraining2022,
  title = {{{LayoutLMv2}}: {{Multi-modal Pre-training}} for {{Visually-Rich Document Understanding}}},
  shorttitle = {{{LayoutLMv2}}},
  author = {Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and Zhang, Min and Zhou, Lidong},
  year = {2022},
  month = jan,
  journal = {arXiv:2012.14740 [cs]},
  eprint = {2012.14740},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-ofthe-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 \textrightarrow{} 0.8420), CORD (0.9493 \textrightarrow{} 0.9601), SROIE (0.9524 \textrightarrow{} 0.9781), Kleister-NDA (0.8340 \textrightarrow{} 0.8520), RVL-CDIP (0.9443 \textrightarrow{} 0.9564), and DocVQA (0.7295 \textrightarrow{} 0.8672). We made our model and code publicly available at https://aka.ms /layoutlmv2.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/dimdakis/Zotero/storage/UPMSVEYX/Xu et al. - 2022 - LayoutLMv2 Multi-modal Pre-training for Visually-.pdf}
}


